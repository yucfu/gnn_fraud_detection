{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, GATConv, DenseGCNConv, GCNConv, GraphConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ref: https://www.kaggle.com/code/divyareddyyeruva/elliptic-gcn-pyg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171469</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>230425980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5530458</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>5530458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232022460</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>232022460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>232438397</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782742</td>\n",
       "      <td>12.414558</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>232438397</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230460314</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "      <td>230460314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1         2         3         4          5         6         7  \\\n",
       "0  230425980  1 -0.171469 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "1    5530458  1 -0.171484 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "2  232022460  1 -0.172107 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "3  232438397  1  0.163054  1.963790 -0.646376  12.409294 -0.063725  9.782742   \n",
       "4  230460314  1  1.011523 -0.081127 -1.201369   1.153668  0.333276  1.312656   \n",
       "\n",
       "           8         9  ...       159       160       161       162       163  \\\n",
       "0  -0.061584 -0.162097  ...  1.461330  1.461369  0.018279 -0.087490 -0.131155   \n",
       "1  -0.061584 -0.162112  ... -0.979074 -0.978556  0.018279 -0.087490 -0.131155   \n",
       "2  -0.061584 -0.162749  ... -0.979074 -0.978556 -0.098889 -0.106715 -0.131155   \n",
       "3  12.414558 -0.163645  ...  0.241128  0.241406  1.072793  0.085530 -0.131155   \n",
       "4  -0.061584 -0.163523  ...  0.517257  0.579382  0.018279  0.277775  0.326394   \n",
       "\n",
       "        164       165       166       txId  class  \n",
       "0 -0.097524 -0.120613 -0.119792  230425980      0  \n",
       "1 -0.097524 -0.120613 -0.119792    5530458      0  \n",
       "2 -0.183671 -0.120613 -0.119792  232022460      0  \n",
       "3  0.677799 -0.120613 -0.119792  232438397      2  \n",
       "4  1.293750  0.178136  0.179117  230460314      0  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import data \n",
    "df_features = pd.read_csv('data/elliptic_txs_features.csv', header=None)\n",
    "df_edges = pd.read_csv(\"data/elliptic_txs_edgelist.csv\")\n",
    "df_classes =  pd.read_csv(\"data/elliptic_txs_classes.csv\")\n",
    "# map unknown classes to -1\n",
    "df_classes['class'] = df_classes['class'].apply(lambda x: 0 if x == \"unknown\" else int(x))\n",
    "\n",
    "# merging dataframes\n",
    "df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
    "display(df_merge.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230425980</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171469</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>230425980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5530458</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>5530458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>232022460</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>232022460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>232438397</td>\n",
       "      <td>1</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782742</td>\n",
       "      <td>12.414558</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>232438397</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230460314</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "      <td>230460314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0  1         2         3         4          5         6         7  \\\n",
       "0  230425980  1 -0.171469 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "1    5530458  1 -0.171484 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "2  232022460  1 -0.172107 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "3  232438397  1  0.163054  1.963790 -0.646376  12.409294 -0.063725  9.782742   \n",
       "4  230460314  1  1.011523 -0.081127 -1.201369   1.153668  0.333276  1.312656   \n",
       "\n",
       "           8         9  ...       159       160       161       162       163  \\\n",
       "0  -0.061584 -0.162097  ...  1.461330  1.461369  0.018279 -0.087490 -0.131155   \n",
       "1  -0.061584 -0.162112  ... -0.979074 -0.978556  0.018279 -0.087490 -0.131155   \n",
       "2  -0.061584 -0.162749  ... -0.979074 -0.978556 -0.098889 -0.106715 -0.131155   \n",
       "3  12.414558 -0.163645  ...  0.241128  0.241406  1.072793  0.085530 -0.131155   \n",
       "4  -0.061584 -0.163523  ...  0.517257  0.579382  0.018279  0.277775  0.326394   \n",
       "\n",
       "        164       165       166       txId  class  \n",
       "0 -0.097524 -0.120613 -0.119792  230425980      0  \n",
       "1 -0.097524 -0.120613 -0.119792    5530458      0  \n",
       "2 -0.183671 -0.120613 -0.119792  232022460      0  \n",
       "3  0.677799 -0.120613 -0.119792  232438397      2  \n",
       "4  1.293750  0.178136  0.179117  230460314      0  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136265</th>\n",
       "      <td>54785412</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.159837</td>\n",
       "      <td>-0.030732</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.150191</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>54785412</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136266</th>\n",
       "      <td>69354384</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.165893</td>\n",
       "      <td>-0.029572</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.156388</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>69354384</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136267</th>\n",
       "      <td>54775772</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.129693</td>\n",
       "      <td>0.070098</td>\n",
       "      <td>1.573595</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>0.075226</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.119348</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.463356</td>\n",
       "      <td>-0.462939</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>54775772</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136268</th>\n",
       "      <td>69343934</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.111789</td>\n",
       "      <td>1.294910</td>\n",
       "      <td>1.573595</td>\n",
       "      <td>0.553368</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>0.641758</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.159732</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>69343934</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136269</th>\n",
       "      <td>70102750</td>\n",
       "      <td>35</td>\n",
       "      <td>-0.172796</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163571</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.854508</td>\n",
       "      <td>2.146417</td>\n",
       "      <td>2.013077</td>\n",
       "      <td>-1.760926</td>\n",
       "      <td>-1.760984</td>\n",
       "      <td>70102750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0   1         2         3         4         5         6  \\\n",
       "136265  54785412  35 -0.159837 -0.030732  1.018602 -0.121970 -0.043875   \n",
       "136266  69354384  35 -0.165893 -0.029572  1.018602 -0.121970 -0.043875   \n",
       "136267  54775772  35 -0.129693  0.070098  1.573595 -0.121970  0.075226   \n",
       "136268  69343934  35 -0.111789  1.294910  1.573595  0.553368 -0.043875   \n",
       "136269  70102750  35 -0.172796 -0.081127 -1.201369 -0.046932 -0.043875   \n",
       "\n",
       "               7         8         9  ...       159       160       161  \\\n",
       "136265 -0.113002 -0.061584 -0.150191  ... -0.979074 -0.978556  0.018279   \n",
       "136266 -0.113002 -0.061584 -0.156388  ...  1.461330  1.461369  0.018279   \n",
       "136267 -0.113002 -0.061584 -0.119348  ... -0.463356 -0.462939  0.018279   \n",
       "136268  0.641758 -0.061584 -0.159732  ...  1.461330  1.461369  0.018279   \n",
       "136269 -0.029140 -0.061584 -0.163571  ... -0.979074 -0.978556  0.018279   \n",
       "\n",
       "             162       163       164       165       166      txId  class  \n",
       "136265 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  54785412      0  \n",
       "136266 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  69354384      0  \n",
       "136267 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  54775772      0  \n",
       "136268 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792  69343934      0  \n",
       "136269  0.854508  2.146417  2.013077 -1.760926 -1.760984  70102750      0  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take time step from 1 to 34 as train data\n",
    "df_train = df_merge[df_merge[1] <= 34]\n",
    "# take rest as test data\n",
    "df_test = df_merge[df_merge[1] > 34]\n",
    "display(df_train.head())\n",
    "display(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape:  (136265, 94)\n",
      "Train labels shape:  (136265,)\n",
      "Test features shape:  (67504, 94)\n",
      "Test labels shape:  (67504,)\n"
     ]
    }
   ],
   "source": [
    "num_features = 94\n",
    "\n",
    "# split train and test features\n",
    "train_features = df_train.iloc[:, 2:2+num_features].values\n",
    "test_features = df_test.iloc[:, 2:2+num_features].values\n",
    "\n",
    "# split train and test labels\n",
    "train_labels = df_train.iloc[:, -1].values\n",
    "test_labels = df_test.iloc[:, -1].values\n",
    "\n",
    "print(\"Train features shape: \", train_features.shape)\n",
    "print(\"Train labels shape: \", train_labels.shape)\n",
    "\n",
    "print(\"Test features shape: \", test_features.shape)\n",
    "print(\"Test labels shape: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 234355])\n"
     ]
    }
   ],
   "source": [
    "# all nodes in data\n",
    "nodes = df_merge[0].values\n",
    "map_id = {j:i for i,j in enumerate(nodes)} # mapping nodes to indexes\n",
    "\n",
    "edges = df_edges.copy()\n",
    "edges.txId1 = edges.txId1.map(map_id)\n",
    "edges.txId2 = edges.txId2.map(map_id)\n",
    "edges = edges.astype(int)\n",
    "\n",
    "edge_index = np.array(edges.values).T\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
    "edge_weight = torch.tensor([1]* edge_index.shape[1] , dtype=torch.double)\n",
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nodes = df_train[0].unique()\n",
    "map_id = {j:i for i,j in enumerate(nodes)}\n",
    "train_idx = [map_id[node_id] for node_id in train_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 156843])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_edge_mask = edges['txId1'].isin(train_idx) & edges['txId2'].isin(train_idx)\n",
    "train_edge_index = edge_index[:, train_edge_mask]\n",
    "train_edge_weight = edge_weight[train_edge_mask]\n",
    "train_edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 77512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inverse mapping for test data\n",
    "test_nodes = df_test[0].unique()\n",
    "map_id = {j:i for i,j in enumerate(nodes)}\n",
    "test_idx = [map_id[node_id] for node_id in test_nodes]\n",
    "\n",
    "test_edge_mask = edges['txId1'].isin(test_idx) & edges['txId2'].isin(test_idx)\n",
    "test_edge_index = edge_index[:, test_edge_mask]\n",
    "test_edge_weight = edge_weight[test_edge_mask]\n",
    "test_edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_index = test_edge_index - 136265\n",
    "new_test_index = new_test_index.long()\n",
    "type(new_test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[136265, 94], edge_index=[2, 156843], y=[136265], edge_weight=[156843])\n",
      "Data(x=[67504, 94], edge_index=[2], y=[67504], edge_weight=[77512])\n"
     ]
    }
   ],
   "source": [
    "# construct graph train data and test data\n",
    "train_graph = Data(x=torch.tensor(train_features, dtype=torch.double), edge_index=train_edge_index, edge_weight=train_edge_weight, y=torch.tensor(train_labels, dtype=torch.long))\n",
    "# train_graph = Data(x=torch.tensor(train_features, dtype=torch.float), edge_index=edge_index, edge_weight=edge_weight, y=torch.tensor(train_labels, dtype=torch.double))\n",
    "test_graph = Data(x=torch.tensor(test_features, dtype=torch.double), edge_index=new_test_index, edge_weight=test_edge_weight, y=torch.tensor(test_labels, dtype=torch.long))\n",
    "print(train_graph)\n",
    "print(test_graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels=128):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight = data.x, data.edge_index, data.edge_weight\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.cross_entropy(out, data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # logits = model(data.x, data.edge_index)\n",
    "        logits = model(data)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = accuracy_score(data.y.cpu(), preds.cpu())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(94, 128)\n",
      "  (conv2): GCNConv(128, 3)\n",
      ")\n",
      "Epoch: 1, Train Loss: 1.2579, \n",
      "Epoch: 2, Train Loss: 0.7184, \n",
      "Epoch: 3, Train Loss: 0.6313, \n",
      "Epoch: 4, Train Loss: 0.6341, \n",
      "Epoch: 5, Train Loss: 0.6375, \n",
      "Epoch: 6, Train Loss: 0.6326, \n",
      "Epoch: 7, Train Loss: 0.6004, \n",
      "Epoch: 8, Train Loss: 0.5883, \n",
      "Epoch: 9, Train Loss: 0.5670, \n",
      "Epoch: 10, Train Loss: 0.5450, \n",
      "Epoch: 11, Train Loss: 0.5433, \n",
      "Epoch: 12, Train Loss: 0.5336, \n",
      "Epoch: 13, Train Loss: 0.5286, \n",
      "Epoch: 14, Train Loss: 0.5244, \n",
      "Epoch: 15, Train Loss: 0.5150, \n",
      "Epoch: 16, Train Loss: 0.5104, \n",
      "Epoch: 17, Train Loss: 0.5072, \n",
      "Epoch: 18, Train Loss: 0.5038, \n",
      "Epoch: 19, Train Loss: 0.4998, \n",
      "Epoch: 20, Train Loss: 0.4959, \n",
      "Epoch: 21, Train Loss: 0.4949, \n",
      "Epoch: 22, Train Loss: 0.4921, \n",
      "Epoch: 23, Train Loss: 0.4889, \n",
      "Epoch: 24, Train Loss: 0.4873, \n",
      "Epoch: 25, Train Loss: 0.4890, \n",
      "Epoch: 26, Train Loss: 0.4830, \n",
      "Epoch: 27, Train Loss: 0.4799, \n",
      "Epoch: 28, Train Loss: 0.4800, \n",
      "Epoch: 29, Train Loss: 0.4768, \n",
      "Epoch: 30, Train Loss: 0.4775, \n",
      "Epoch: 31, Train Loss: 0.4761, \n",
      "Epoch: 32, Train Loss: 0.4743, \n",
      "Epoch: 33, Train Loss: 0.4725, \n",
      "Epoch: 34, Train Loss: 0.4712, \n",
      "Epoch: 35, Train Loss: 0.4713, \n",
      "Epoch: 36, Train Loss: 0.4691, \n",
      "Epoch: 37, Train Loss: 0.4675, \n",
      "Epoch: 38, Train Loss: 0.4669, \n",
      "Epoch: 39, Train Loss: 0.4659, \n",
      "Epoch: 40, Train Loss: 0.4649, \n",
      "Epoch: 41, Train Loss: 0.4638, \n",
      "Epoch: 42, Train Loss: 0.4632, \n",
      "Epoch: 43, Train Loss: 0.4612, \n",
      "Epoch: 44, Train Loss: 0.4624, \n",
      "Epoch: 45, Train Loss: 0.4605, \n",
      "Epoch: 46, Train Loss: 0.4591, \n",
      "Epoch: 47, Train Loss: 0.4585, \n",
      "Epoch: 48, Train Loss: 0.4578, \n",
      "Epoch: 49, Train Loss: 0.4570, \n",
      "Epoch: 50, Train Loss: 0.4571, \n",
      "Epoch: 51, Train Loss: 0.4555, \n",
      "Epoch: 52, Train Loss: 0.4545, \n",
      "Epoch: 53, Train Loss: 0.4538, \n",
      "Epoch: 54, Train Loss: 0.4528, \n",
      "Epoch: 55, Train Loss: 0.4523, \n",
      "Epoch: 56, Train Loss: 0.4510, \n",
      "Epoch: 57, Train Loss: 0.4514, \n",
      "Epoch: 58, Train Loss: 0.4516, \n",
      "Epoch: 59, Train Loss: 0.4501, \n",
      "Epoch: 60, Train Loss: 0.4489, \n",
      "Epoch: 61, Train Loss: 0.4491, \n",
      "Epoch: 62, Train Loss: 0.4485, \n",
      "Epoch: 63, Train Loss: 0.4479, \n",
      "Epoch: 64, Train Loss: 0.4473, \n",
      "Epoch: 65, Train Loss: 0.4464, \n",
      "Epoch: 66, Train Loss: 0.4458, \n",
      "Epoch: 67, Train Loss: 0.4459, \n",
      "Epoch: 68, Train Loss: 0.4453, \n",
      "Epoch: 69, Train Loss: 0.4443, \n",
      "Epoch: 70, Train Loss: 0.4452, \n",
      "Epoch: 71, Train Loss: 0.4439, \n",
      "Epoch: 72, Train Loss: 0.4436, \n",
      "Epoch: 73, Train Loss: 0.4420, \n",
      "Epoch: 74, Train Loss: 0.4416, \n",
      "Epoch: 75, Train Loss: 0.4416, \n",
      "Epoch: 76, Train Loss: 0.4411, \n",
      "Epoch: 77, Train Loss: 0.4405, \n",
      "Epoch: 78, Train Loss: 0.4402, \n",
      "Epoch: 79, Train Loss: 0.4399, \n",
      "Epoch: 80, Train Loss: 0.4400, \n",
      "Epoch: 81, Train Loss: 0.4395, \n",
      "Epoch: 82, Train Loss: 0.4383, \n",
      "Epoch: 83, Train Loss: 0.4379, \n",
      "Epoch: 84, Train Loss: 0.4380, \n",
      "Epoch: 85, Train Loss: 0.4381, \n",
      "Epoch: 86, Train Loss: 0.4366, \n",
      "Epoch: 87, Train Loss: 0.4358, \n",
      "Epoch: 88, Train Loss: 0.4355, \n",
      "Epoch: 89, Train Loss: 0.4349, \n",
      "Epoch: 90, Train Loss: 0.4360, \n",
      "Epoch: 91, Train Loss: 0.4339, \n",
      "Epoch: 92, Train Loss: 0.4342, \n",
      "Epoch: 93, Train Loss: 0.4331, \n",
      "Epoch: 94, Train Loss: 0.4328, \n",
      "Epoch: 95, Train Loss: 0.4329, \n",
      "Epoch: 96, Train Loss: 0.4324, \n",
      "Epoch: 97, Train Loss: 0.4321, \n",
      "Epoch: 98, Train Loss: 0.4319, \n",
      "Epoch: 99, Train Loss: 0.4328, \n",
      "Epoch: 100, Train Loss: 0.4310, \n",
      "Epoch: 101, Train Loss: 0.4302, \n",
      "Epoch: 102, Train Loss: 0.4306, \n",
      "Epoch: 103, Train Loss: 0.4296, \n",
      "Epoch: 104, Train Loss: 0.4286, \n",
      "Epoch: 105, Train Loss: 0.4292, \n",
      "Epoch: 106, Train Loss: 0.4292, \n",
      "Epoch: 107, Train Loss: 0.4287, \n",
      "Epoch: 108, Train Loss: 0.4279, \n",
      "Epoch: 109, Train Loss: 0.4274, \n",
      "Epoch: 110, Train Loss: 0.4266, \n",
      "Epoch: 111, Train Loss: 0.4270, \n",
      "Epoch: 112, Train Loss: 0.4271, \n",
      "Epoch: 113, Train Loss: 0.4259, \n",
      "Epoch: 114, Train Loss: 0.4251, \n",
      "Epoch: 115, Train Loss: 0.4264, \n",
      "Epoch: 116, Train Loss: 0.4249, \n",
      "Epoch: 117, Train Loss: 0.4256, \n",
      "Epoch: 118, Train Loss: 0.4243, \n",
      "Epoch: 119, Train Loss: 0.4247, \n",
      "Epoch: 120, Train Loss: 0.4245, \n",
      "Epoch: 121, Train Loss: 0.4241, \n",
      "Epoch: 122, Train Loss: 0.4232, \n",
      "Epoch: 123, Train Loss: 0.4231, \n",
      "Epoch: 124, Train Loss: 0.4231, \n",
      "Epoch: 125, Train Loss: 0.4229, \n",
      "Epoch: 126, Train Loss: 0.4213, \n",
      "Epoch: 127, Train Loss: 0.4222, \n",
      "Epoch: 128, Train Loss: 0.4213, \n",
      "Epoch: 129, Train Loss: 0.4210, \n",
      "Epoch: 130, Train Loss: 0.4214, \n",
      "Epoch: 131, Train Loss: 0.4204, \n",
      "Epoch: 132, Train Loss: 0.4212, \n",
      "Epoch: 133, Train Loss: 0.4201, \n",
      "Epoch: 134, Train Loss: 0.4200, \n",
      "Epoch: 135, Train Loss: 0.4193, \n",
      "Epoch: 136, Train Loss: 0.4200, \n",
      "Epoch: 137, Train Loss: 0.4189, \n",
      "Epoch: 138, Train Loss: 0.4194, \n",
      "Epoch: 139, Train Loss: 0.4180, \n",
      "Epoch: 140, Train Loss: 0.4179, \n",
      "Epoch: 141, Train Loss: 0.4169, \n",
      "Epoch: 142, Train Loss: 0.4172, \n",
      "Epoch: 143, Train Loss: 0.4175, \n",
      "Epoch: 144, Train Loss: 0.4171, \n",
      "Epoch: 145, Train Loss: 0.4165, \n",
      "Epoch: 146, Train Loss: 0.4150, \n",
      "Epoch: 147, Train Loss: 0.4168, \n",
      "Epoch: 148, Train Loss: 0.4151, \n",
      "Epoch: 149, Train Loss: 0.4143, \n",
      "Epoch: 150, Train Loss: 0.4140, \n",
      "Epoch: 151, Train Loss: 0.4147, \n",
      "Epoch: 152, Train Loss: 0.4146, \n",
      "Epoch: 153, Train Loss: 0.4145, \n",
      "Epoch: 154, Train Loss: 0.4138, \n",
      "Epoch: 155, Train Loss: 0.4140, \n",
      "Epoch: 156, Train Loss: 0.4140, \n",
      "Epoch: 157, Train Loss: 0.4131, \n",
      "Epoch: 158, Train Loss: 0.4119, \n",
      "Epoch: 159, Train Loss: 0.4133, \n",
      "Epoch: 160, Train Loss: 0.4127, \n",
      "Epoch: 161, Train Loss: 0.4123, \n",
      "Epoch: 162, Train Loss: 0.4122, \n",
      "Epoch: 163, Train Loss: 0.4132, \n",
      "Epoch: 164, Train Loss: 0.4117, \n",
      "Epoch: 165, Train Loss: 0.4116, \n",
      "Epoch: 166, Train Loss: 0.4112, \n",
      "Epoch: 167, Train Loss: 0.4109, \n",
      "Epoch: 168, Train Loss: 0.4106, \n",
      "Epoch: 169, Train Loss: 0.4118, \n",
      "Epoch: 170, Train Loss: 0.4114, \n",
      "Epoch: 171, Train Loss: 0.4095, \n",
      "Epoch: 172, Train Loss: 0.4103, \n",
      "Epoch: 173, Train Loss: 0.4099, \n",
      "Epoch: 174, Train Loss: 0.4105, \n",
      "Epoch: 175, Train Loss: 0.4099, \n",
      "Epoch: 176, Train Loss: 0.4104, \n",
      "Epoch: 177, Train Loss: 0.4098, \n",
      "Epoch: 178, Train Loss: 0.4081, \n",
      "Epoch: 179, Train Loss: 0.4089, \n",
      "Epoch: 180, Train Loss: 0.4095, \n",
      "Epoch: 181, Train Loss: 0.4082, \n",
      "Epoch: 182, Train Loss: 0.4083, \n",
      "Epoch: 183, Train Loss: 0.4076, \n",
      "Epoch: 184, Train Loss: 0.4077, \n",
      "Epoch: 185, Train Loss: 0.4075, \n",
      "Epoch: 186, Train Loss: 0.4078, \n",
      "Epoch: 187, Train Loss: 0.4077, \n",
      "Epoch: 188, Train Loss: 0.4070, \n",
      "Epoch: 189, Train Loss: 0.4072, \n",
      "Epoch: 190, Train Loss: 0.4068, \n",
      "Epoch: 191, Train Loss: 0.4066, \n",
      "Epoch: 192, Train Loss: 0.4070, \n",
      "Epoch: 193, Train Loss: 0.4057, \n",
      "Epoch: 194, Train Loss: 0.4060, \n",
      "Epoch: 195, Train Loss: 0.4049, \n",
      "Epoch: 196, Train Loss: 0.4052, \n",
      "Epoch: 197, Train Loss: 0.4049, \n",
      "Epoch: 198, Train Loss: 0.4049, \n",
      "Epoch: 199, Train Loss: 0.4052, \n",
      "Epoch: 200, Train Loss: 0.4055, \n"
     ]
    }
   ],
   "source": [
    "model = GCN(num_features, 3).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "num_epochs = 200\n",
    "print(model)\n",
    "\n",
    "# for epoch in tqdm(range(1, num_epochs+1)):\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    model = model.double()\n",
    "    train_loss = train(model, train_graph.to(device), optimizer)\n",
    "    # train_acc = test(model, train_graph)\n",
    "    # test_acc = test(model, test_graph)\n",
    "    # if epoch % 10 == 0:\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`MessagePassing.propagate` only supports integer tensors of shape `[2, num_messages]`, `torch_sparse.SparseTensor` or `torch.sparse.Tensor` for argument `edge_index`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# test accuracy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_acc \u001b[39m=\u001b[39m test(model, test_graph\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTest Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, data)\u001b[0m\n\u001b[1;32m     11\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     12\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     13\u001b[0m     \u001b[39m# logits = model(data.x, data.edge_index)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     logits \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     15\u001b[0m     preds \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m     acc \u001b[39m=\u001b[39m accuracy_score(data\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mcpu(), preds\u001b[39m.\u001b[39mcpu())\n",
      "File \u001b[0;32m~/mambaforge/envs/gnn/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[10], line 11\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[1;32m     10\u001b[0m     x, edge_index, edge_weight \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mx, data\u001b[39m.\u001b[39medge_index, data\u001b[39m.\u001b[39medge_weight\n\u001b[0;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index, edge_weight)\n\u001b[1;32m     12\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[1;32m     13\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/mambaforge/envs/gnn/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/gnn/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:232\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    229\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(x)\n\u001b[1;32m    231\u001b[0m \u001b[39m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, edge_weight\u001b[39m=\u001b[39;49medge_weight,\n\u001b[1;32m    233\u001b[0m                      size\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    236\u001b[0m     out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\n",
      "File \u001b[0;32m~/mambaforge/envs/gnn/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:422\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    420\u001b[0m         edge_index, size, kwargs \u001b[39m=\u001b[39m res\n\u001b[0;32m--> 422\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input(edge_index, size)\n\u001b[1;32m    424\u001b[0m \u001b[39m# Run \"fused\" message and aggregation (if applicable).\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m is_sparse(edge_index) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuse \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplain:\n",
      "File \u001b[0;32m~/mambaforge/envs/gnn/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:235\u001b[0m, in \u001b[0;36mMessagePassing._check_input\u001b[0;34m(self, edge_index, size)\u001b[0m\n\u001b[1;32m    232\u001b[0m         the_size[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m size[\u001b[39m1\u001b[39m]\n\u001b[1;32m    233\u001b[0m     \u001b[39mreturn\u001b[39;00m the_size\n\u001b[0;32m--> 235\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39m`MessagePassing.propagate` only supports integer tensors of \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    237\u001b[0m      \u001b[39m'\u001b[39m\u001b[39mshape `[2, num_messages]`, `torch_sparse.SparseTensor` or \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    238\u001b[0m      \u001b[39m'\u001b[39m\u001b[39m`torch.sparse.Tensor` for argument `edge_index`.\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: `MessagePassing.propagate` only supports integer tensors of shape `[2, num_messages]`, `torch_sparse.SparseTensor` or `torch.sparse.Tensor` for argument `edge_index`."
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "test_acc = test(model, test_graph.to(device))\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
