{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from dgl.data.utils import load_graphs, save_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dgl\n",
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nodes = pd.read_csv('data/tfinance/selected_nodes.csv')\n",
    "selected_edges = pd.read_csv('data/tfinance/selected_edges.csv')\n",
    "selected_labels = pd.read_csv('data/tfinance/selected_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, GATConv, DenseGCNConv, GCNConv, GraphConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.loader import DataLoader as pyg_DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966892</td>\n",
       "      <td>-0.302345</td>\n",
       "      <td>-0.303793</td>\n",
       "      <td>-0.177117</td>\n",
       "      <td>-0.267849</td>\n",
       "      <td>-0.161530</td>\n",
       "      <td>-0.364640</td>\n",
       "      <td>-0.142817</td>\n",
       "      <td>-0.417713</td>\n",
       "      <td>-0.643477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.975264</td>\n",
       "      <td>-0.182326</td>\n",
       "      <td>1.641039</td>\n",
       "      <td>0.465295</td>\n",
       "      <td>-0.268168</td>\n",
       "      <td>0.357262</td>\n",
       "      <td>-0.458247</td>\n",
       "      <td>0.386965</td>\n",
       "      <td>-0.894606</td>\n",
       "      <td>-0.848755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.541011</td>\n",
       "      <td>-0.278478</td>\n",
       "      <td>0.016163</td>\n",
       "      <td>-0.102169</td>\n",
       "      <td>-0.228462</td>\n",
       "      <td>-0.019719</td>\n",
       "      <td>0.033190</td>\n",
       "      <td>-0.022844</td>\n",
       "      <td>-0.875343</td>\n",
       "      <td>-0.872905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.031149</td>\n",
       "      <td>-0.253015</td>\n",
       "      <td>-0.234783</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.134698</td>\n",
       "      <td>-0.192256</td>\n",
       "      <td>-0.516752</td>\n",
       "      <td>-0.160635</td>\n",
       "      <td>-0.226134</td>\n",
       "      <td>-0.688734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.275260</td>\n",
       "      <td>-0.281746</td>\n",
       "      <td>-0.278698</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>-0.209167</td>\n",
       "      <td>-0.160348</td>\n",
       "      <td>-0.271033</td>\n",
       "      <td>-0.152320</td>\n",
       "      <td>-0.409034</td>\n",
       "      <td>-0.404129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>-0.480986</td>\n",
       "      <td>-0.303485</td>\n",
       "      <td>-0.146952</td>\n",
       "      <td>-0.123583</td>\n",
       "      <td>-0.269444</td>\n",
       "      <td>-0.144986</td>\n",
       "      <td>-0.446546</td>\n",
       "      <td>-0.117872</td>\n",
       "      <td>0.729831</td>\n",
       "      <td>1.183965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>-0.929844</td>\n",
       "      <td>0.330664</td>\n",
       "      <td>-0.297519</td>\n",
       "      <td>-0.252065</td>\n",
       "      <td>1.094280</td>\n",
       "      <td>-0.215891</td>\n",
       "      <td>-0.516752</td>\n",
       "      <td>-0.184392</td>\n",
       "      <td>-0.859675</td>\n",
       "      <td>-0.864036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>-0.540210</td>\n",
       "      <td>-0.197832</td>\n",
       "      <td>0.185552</td>\n",
       "      <td>-0.102169</td>\n",
       "      <td>-0.268965</td>\n",
       "      <td>0.040551</td>\n",
       "      <td>-0.411444</td>\n",
       "      <td>0.072184</td>\n",
       "      <td>-0.205254</td>\n",
       "      <td>-0.243863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>-0.845683</td>\n",
       "      <td>0.236032</td>\n",
       "      <td>-0.215962</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>-0.240421</td>\n",
       "      <td>-0.076443</td>\n",
       "      <td>-0.399743</td>\n",
       "      <td>-0.045413</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>0.183068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>-0.367213</td>\n",
       "      <td>-0.244045</td>\n",
       "      <td>2.381330</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.253657</td>\n",
       "      <td>0.217815</td>\n",
       "      <td>-0.481649</td>\n",
       "      <td>0.250362</td>\n",
       "      <td>-0.575189</td>\n",
       "      <td>-0.778838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0      0.966892 -0.302345 -0.303793 -0.177117 -0.267849 -0.161530 -0.364640   \n",
       "1      1.975264 -0.182326  1.641039  0.465295 -0.268168  0.357262 -0.458247   \n",
       "2      2.541011 -0.278478  0.016163 -0.102169 -0.228462 -0.019719  0.033190   \n",
       "3     -1.031149 -0.253015 -0.234783 -0.305600 -0.134698 -0.192256 -0.516752   \n",
       "4     -0.275260 -0.281746 -0.278698 -0.230652 -0.209167 -0.160348 -0.271033   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14995 -0.480986 -0.303485 -0.146952 -0.123583 -0.269444 -0.144986 -0.446546   \n",
       "14996 -0.929844  0.330664 -0.297519 -0.252065  1.094280 -0.215891 -0.516752   \n",
       "14997 -0.540210 -0.197832  0.185552 -0.102169 -0.268965  0.040551 -0.411444   \n",
       "14998 -0.845683  0.236032 -0.215962 -0.230652 -0.240421 -0.076443 -0.399743   \n",
       "14999 -0.367213 -0.244045  2.381330 -0.305600 -0.253657  0.217815 -0.481649   \n",
       "\n",
       "       feature8  feature9  feature10  \n",
       "0     -0.142817 -0.417713  -0.643477  \n",
       "1      0.386965 -0.894606  -0.848755  \n",
       "2     -0.022844 -0.875343  -0.872905  \n",
       "3     -0.160635 -0.226134  -0.688734  \n",
       "4     -0.152320 -0.409034  -0.404129  \n",
       "...         ...       ...        ...  \n",
       "14995 -0.117872  0.729831   1.183965  \n",
       "14996 -0.184392 -0.859675  -0.864036  \n",
       "14997  0.072184 -0.205254  -0.243863  \n",
       "14998 -0.045413  0.264550   0.183068  \n",
       "14999  0.250362 -0.575189  -0.778838  \n",
       "\n",
       "[15000 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(selected_nodes)\n",
    "selected_nodes_scaled = pd.DataFrame(features_scaled, index=selected_nodes.index, columns=selected_nodes.columns)\n",
    "selected_nodes_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12230, 12230, 12230,  ..., 10945, 10945, 10945],\n",
      "        [ 6579,  8455,   272,  ...,  8153,  6480,  9753]])\n",
      "shape of edge index is torch.Size([2, 6032438])\n"
     ]
    }
   ],
   "source": [
    "edge_index = np.array(selected_edges.values).T \n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "print(edge_index)\n",
    "\n",
    "print(\"shape of edge index is {}\".format(edge_index.shape))\n",
    "\n",
    "node_features_t = torch.tensor(np.array(selected_nodes.values, dtype=np.double), dtype=torch.double)\n",
    "data_graph = Data(x=node_features_t.float(), edge_index=edge_index,\n",
    "                               y=torch.tensor(selected_labels.values.flatten(), dtype=torch.long))\n",
    "\n",
    "node_features_scaled_t = torch.tensor(np.array(selected_nodes_scaled.values, dtype=np.double), dtype=torch.double)\n",
    "data_graph_scaled = Data(x=node_features_scaled_t.float(), edge_index=edge_index,\n",
    "                                 y=torch.tensor(selected_labels.values.flatten(), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 15000\n",
    "\n",
    "train_idx = list(pd.read_csv('data/tfinance/index/train_idx.csv', names=['id']).values.flatten())\n",
    "test_idx = list(pd.read_csv('data/tfinance/index/test_idx.csv', names=['id']).values.flatten())\n",
    "\n",
    "data_graph.train_idx = torch.zeros(sample_size, dtype=torch.bool)\n",
    "data_graph.train_idx[train_idx] = 1\n",
    "\n",
    "data_graph.test_idx = torch.zeros(sample_size, dtype=torch.bool)\n",
    "data_graph.test_idx[test_idx] = 1\n",
    "\n",
    "data_graph_scaled.train_idx = torch.zeros(sample_size, dtype=torch.bool)\n",
    "data_graph_scaled.train_idx[train_idx] = 1\n",
    "\n",
    "data_graph_scaled.test_idx = torch.zeros(sample_size, dtype=torch.bool)\n",
    "data_graph_scaled.test_idx[test_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels=128):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS_WEIGTHS = [0.7,0.3]\n",
    "\n",
    "def train(model, data, optimizer, scheduler):\n",
    "    model.train()\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    # out = out.reshape((data.x.shape[0]))\n",
    "    # # use weighted cross entropy loss, weighted cross entropy loss to provide higher importance to the illicit samples.\n",
    "    # weights = torch.tensor(CLASS_WEIGTHS, dtype=torch.float).to(device)\n",
    "    # loss = F.cross_entropy(out[data.train_idx], data.y[data.train_idx], weight=weights)\n",
    "    \n",
    "    loss = F.cross_entropy(out[data.train_idx], data.y[data.train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    return loss\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred_scores = out[data.test_idx]\n",
    "        pred = torch.argmax(pred_scores, dim=1)\n",
    "        y = data.y[data.test_idx]\n",
    "        # metrics for illicit transactions\n",
    "        acc = accuracy_score(y.cpu(), pred.cpu())\n",
    "        f1 = f1_score(y.cpu(), pred.cpu(), average='binary')\n",
    "        precision = precision_score(y.cpu(), pred.cpu(), average='binary')\n",
    "        recall = recall_score(y.cpu(), pred.cpu(), average='binary')\n",
    "        auc = roc_auc_score(y.cpu(), pred_scores[:,1].cpu())\n",
    "        return acc, f1, precision, recall, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features= 10\n"
     ]
    }
   ],
   "source": [
    "num_features = data_graph.num_node_features\n",
    "print(\"num_features=\",num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.6644, Accuracy: 0.8956, F1: 0.3974, Precision:  0.2748, Recall: 0.7176, ROC: 0.6940\n",
      "Epoch: 010, Loss: 0.1534, Accuracy: 0.9713, F1: 0.6921, Precision:  0.7143, Recall: 0.6713, ROC: 0.9079\n",
      "Epoch: 020, Loss: 0.1154, Accuracy: 0.9762, F1: 0.7147, Precision:  0.8428, Recall: 0.6204, ROC: 0.9159\n",
      "Epoch: 030, Loss: 0.1134, Accuracy: 0.9769, F1: 0.7219, Precision:  0.8544, Recall: 0.6250, ROC: 0.9155\n",
      "Epoch: 040, Loss: 0.1121, Accuracy: 0.9767, F1: 0.7287, Precision:  0.8246, Recall: 0.6528, ROC: 0.9143\n",
      "Epoch: 050, Loss: 0.1098, Accuracy: 0.9773, F1: 0.7316, Precision:  0.8476, Recall: 0.6435, ROC: 0.9154\n",
      "Epoch: 060, Loss: 0.1088, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9161\n",
      "Epoch: 070, Loss: 0.1081, Accuracy: 0.9773, F1: 0.7316, Precision:  0.8476, Recall: 0.6435, ROC: 0.9161\n",
      "Epoch: 080, Loss: 0.1076, Accuracy: 0.9776, F1: 0.7349, Precision:  0.8485, Recall: 0.6481, ROC: 0.9163\n",
      "Epoch: 090, Loss: 0.1070, Accuracy: 0.9776, F1: 0.7349, Precision:  0.8485, Recall: 0.6481, ROC: 0.9165\n",
      "Epoch: 100, Loss: 0.1065, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9165\n",
      "Epoch: 110, Loss: 0.1060, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9166\n",
      "Epoch: 120, Loss: 0.1056, Accuracy: 0.9782, F1: 0.7421, Precision:  0.8598, Recall: 0.6528, ROC: 0.9167\n",
      "Epoch: 130, Loss: 0.1051, Accuracy: 0.9784, F1: 0.7454, Precision:  0.8606, Recall: 0.6574, ROC: 0.9167\n",
      "Epoch: 140, Loss: 0.1047, Accuracy: 0.9787, F1: 0.7474, Precision:  0.8659, Recall: 0.6574, ROC: 0.9167\n",
      "Epoch: 150, Loss: 0.1043, Accuracy: 0.9787, F1: 0.7474, Precision:  0.8659, Recall: 0.6574, ROC: 0.9168\n",
      "Epoch: 160, Loss: 0.1040, Accuracy: 0.9787, F1: 0.7474, Precision:  0.8659, Recall: 0.6574, ROC: 0.9168\n",
      "Epoch: 170, Loss: 0.1036, Accuracy: 0.9787, F1: 0.7474, Precision:  0.8659, Recall: 0.6574, ROC: 0.9169\n",
      "Epoch: 180, Loss: 0.1033, Accuracy: 0.9784, F1: 0.7454, Precision:  0.8606, Recall: 0.6574, ROC: 0.9170\n",
      "Epoch: 190, Loss: 0.1031, Accuracy: 0.9787, F1: 0.7474, Precision:  0.8659, Recall: 0.6574, ROC: 0.9172\n",
      "Epoch: 200, Loss: 0.1028, Accuracy: 0.9787, F1: 0.7474, Precision:  0.8659, Recall: 0.6574, ROC: 0.9174\n",
      "Epoch: 210, Loss: 0.1026, Accuracy: 0.9784, F1: 0.7441, Precision:  0.8650, Recall: 0.6528, ROC: 0.9176\n",
      "Epoch: 220, Loss: 0.1024, Accuracy: 0.9780, F1: 0.7402, Precision:  0.8545, Recall: 0.6528, ROC: 0.9178\n",
      "Epoch: 230, Loss: 0.1022, Accuracy: 0.9778, F1: 0.7368, Precision:  0.8537, Recall: 0.6481, ROC: 0.9181\n",
      "Epoch: 240, Loss: 0.1021, Accuracy: 0.9778, F1: 0.7368, Precision:  0.8537, Recall: 0.6481, ROC: 0.9183\n",
      "Epoch: 250, Loss: 0.1019, Accuracy: 0.9778, F1: 0.7368, Precision:  0.8537, Recall: 0.6481, ROC: 0.9186\n",
      "Epoch: 260, Loss: 0.1018, Accuracy: 0.9778, F1: 0.7368, Precision:  0.8537, Recall: 0.6481, ROC: 0.9188\n",
      "Epoch: 270, Loss: 0.1016, Accuracy: 0.9778, F1: 0.7368, Precision:  0.8537, Recall: 0.6481, ROC: 0.9190\n",
      "Epoch: 280, Loss: 0.1015, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9192\n",
      "Epoch: 290, Loss: 0.1014, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9194\n",
      "Epoch: 300, Loss: 0.1013, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9197\n",
      "Epoch: 310, Loss: 0.1012, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9198\n",
      "Epoch: 320, Loss: 0.1010, Accuracy: 0.9773, F1: 0.7316, Precision:  0.8476, Recall: 0.6435, ROC: 0.9199\n",
      "Epoch: 330, Loss: 0.1009, Accuracy: 0.9771, F1: 0.7297, Precision:  0.8424, Recall: 0.6435, ROC: 0.9201\n",
      "Epoch: 340, Loss: 0.1008, Accuracy: 0.9769, F1: 0.7263, Precision:  0.8415, Recall: 0.6389, ROC: 0.9202\n",
      "Epoch: 350, Loss: 0.1007, Accuracy: 0.9769, F1: 0.7263, Precision:  0.8415, Recall: 0.6389, ROC: 0.9203\n",
      "Epoch: 360, Loss: 0.1007, Accuracy: 0.9769, F1: 0.7263, Precision:  0.8415, Recall: 0.6389, ROC: 0.9204\n",
      "Epoch: 370, Loss: 0.1006, Accuracy: 0.9769, F1: 0.7263, Precision:  0.8415, Recall: 0.6389, ROC: 0.9205\n",
      "Epoch: 380, Loss: 0.1005, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9205\n",
      "Epoch: 390, Loss: 0.1004, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9206\n",
      "Epoch: 400, Loss: 0.1003, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9206\n",
      "Epoch: 410, Loss: 0.1002, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9207\n",
      "Epoch: 420, Loss: 0.1001, Accuracy: 0.9773, F1: 0.7316, Precision:  0.8476, Recall: 0.6435, ROC: 0.9207\n",
      "Epoch: 430, Loss: 0.1000, Accuracy: 0.9773, F1: 0.7316, Precision:  0.8476, Recall: 0.6435, ROC: 0.9208\n",
      "Epoch: 440, Loss: 0.0999, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9208\n",
      "Epoch: 450, Loss: 0.0998, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9208\n",
      "Epoch: 460, Loss: 0.0998, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9209\n",
      "Epoch: 470, Loss: 0.0997, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9210\n",
      "Epoch: 480, Loss: 0.0996, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9210\n",
      "Epoch: 490, Loss: 0.0995, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9211\n",
      "Epoch: 500, Loss: 0.0994, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9211\n"
     ]
    }
   ],
   "source": [
    "model = GCN(num_features, 2).to(device)\n",
    "num_epochs = 500\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=100, verbose=True)\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    loss = train(model, data_graph_scaled, optimizer, scheduler)\n",
    "    acc, f1, precision, recall, roc = test(model, data_graph_scaled)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {precision: .4f}, Recall: {recall:.4f}, ROC: {roc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = selected_nodes.loc[train_idx]\n",
    "y_train = selected_labels.loc[train_idx]\n",
    "\n",
    "X_test = selected_nodes.loc[test_idx]\n",
    "y_test = selected_labels.loc[test_idx]\n",
    "\n",
    "X_train_scaled = selected_nodes_scaled.loc[train_idx]\n",
    "X_test_scaled = selected_nodes_scaled.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10000\n",
       "1      500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4284\n",
       "1     216\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9806666666666667\n",
      "F1:        0.7679999999999999\n",
      "Precision: 0.9056603773584906\n",
      "Recall:    0.6666666666666666\n",
      "ROC AUC:   0.9467835745409274\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf.fit(X_train.reset_index(drop=True), y_train.reset_index(drop=True))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "rocauc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1:       \", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:   \", recall)\n",
    "print(\"ROC AUC:  \", rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9806666666666667\n",
      "F1:        0.7679999999999999\n",
      "Precision: 0.9056603773584906\n",
      "Recall:    0.6666666666666666\n",
      "ROC AUC:   0.5371148459383753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf.fit(X_train_scaled.reset_index(drop=True), y_train.reset_index(drop=True))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "rocauc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1:       \", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:   \", recall)\n",
    "print(\"ROC AUC:  \", rocauc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, num_heads, hidden_dim):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=num_heads)\n",
    "        self.conv2 = GATConv(hidden_dim * num_heads, num_classes, concat=False)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.6855, Accuracy: 0.9469, F1: 0.2060, Precision:  0.3647, Recall: 0.1435, ROC: 0.8380\n",
      "Epoch: 010, Loss: 0.2026, Accuracy: 0.9356, F1: 0.0136, Precision:  0.0256, Recall: 0.0093, ROC: 0.8434\n",
      "Epoch: 020, Loss: 0.1523, Accuracy: 0.9464, F1: 0.0082, Precision:  0.0370, Recall: 0.0046, ROC: 0.8877\n",
      "Epoch: 030, Loss: 0.1283, Accuracy: 0.9540, F1: 0.1266, Precision:  0.7143, Recall: 0.0694, ROC: 0.9071\n",
      "Epoch: 040, Loss: 0.1192, Accuracy: 0.9682, F1: 0.5402, Precision:  0.8842, Recall: 0.3889, ROC: 0.9150\n",
      "Epoch: 050, Loss: 0.1168, Accuracy: 0.9731, F1: 0.6630, Precision:  0.8322, Recall: 0.5509, ROC: 0.9165\n",
      "Epoch: 060, Loss: 0.1142, Accuracy: 0.9713, F1: 0.6149, Precision:  0.8655, Recall: 0.4769, ROC: 0.9175\n",
      "Epoch: 070, Loss: 0.1124, Accuracy: 0.9722, F1: 0.6246, Precision:  0.8889, Recall: 0.4815, ROC: 0.9190\n",
      "Epoch: 080, Loss: 0.1108, Accuracy: 0.9724, F1: 0.6242, Precision:  0.9035, Recall: 0.4769, ROC: 0.9182\n",
      "Epoch: 090, Loss: 0.1096, Accuracy: 0.9744, F1: 0.6705, Precision:  0.8797, Recall: 0.5417, ROC: 0.9187\n",
      "Epoch: 100, Loss: 0.1086, Accuracy: 0.9758, F1: 0.6964, Precision:  0.8741, Recall: 0.5787, ROC: 0.9194\n",
      "Epoch: 110, Loss: 0.1077, Accuracy: 0.9767, F1: 0.7123, Precision:  0.8725, Recall: 0.6019, ROC: 0.9197\n",
      "Epoch: 120, Loss: 0.1068, Accuracy: 0.9764, F1: 0.7135, Precision:  0.8571, Recall: 0.6111, ROC: 0.9198\n",
      "Epoch: 130, Loss: 0.1060, Accuracy: 0.9762, F1: 0.7162, Precision:  0.8385, Recall: 0.6250, ROC: 0.9195\n",
      "Epoch: 140, Loss: 0.1053, Accuracy: 0.9764, F1: 0.7211, Precision:  0.8354, Recall: 0.6343, ROC: 0.9201\n",
      "Epoch: 150, Loss: 0.1047, Accuracy: 0.9769, F1: 0.7277, Precision:  0.8373, Recall: 0.6435, ROC: 0.9205\n",
      "Epoch: 160, Loss: 0.1041, Accuracy: 0.9773, F1: 0.7316, Precision:  0.8476, Recall: 0.6435, ROC: 0.9206\n",
      "Epoch: 170, Loss: 0.1036, Accuracy: 0.9780, F1: 0.7415, Precision:  0.8503, Recall: 0.6574, ROC: 0.9200\n",
      "Epoch: 180, Loss: 0.1032, Accuracy: 0.9778, F1: 0.7396, Precision:  0.8452, Recall: 0.6574, ROC: 0.9198\n",
      "Epoch: 190, Loss: 0.1028, Accuracy: 0.9776, F1: 0.7377, Precision:  0.8402, Recall: 0.6574, ROC: 0.9201\n",
      "Epoch: 200, Loss: 0.1024, Accuracy: 0.9778, F1: 0.7423, Precision:  0.8372, Recall: 0.6667, ROC: 0.9197\n",
      "Epoch: 210, Loss: 0.1020, Accuracy: 0.9780, F1: 0.7468, Precision:  0.8343, Recall: 0.6759, ROC: 0.9201\n",
      "Epoch: 220, Loss: 0.1015, Accuracy: 0.9782, F1: 0.7500, Precision:  0.8352, Recall: 0.6806, ROC: 0.9205\n",
      "Epoch: 230, Loss: 0.1010, Accuracy: 0.9784, F1: 0.7519, Precision:  0.8400, Recall: 0.6806, ROC: 0.9201\n",
      "Epoch: 240, Loss: 0.1005, Accuracy: 0.9787, F1: 0.7576, Precision:  0.8333, Recall: 0.6944, ROC: 0.9199\n",
      "Epoch: 250, Loss: 0.1001, Accuracy: 0.9784, F1: 0.7581, Precision:  0.8216, Recall: 0.7037, ROC: 0.9192\n",
      "Epoch: 260, Loss: 0.0998, Accuracy: 0.9782, F1: 0.7562, Precision:  0.8172, Recall: 0.7037, ROC: 0.9189\n",
      "Epoch: 270, Loss: 0.0995, Accuracy: 0.9784, F1: 0.7593, Precision:  0.8182, Recall: 0.7083, ROC: 0.9187\n",
      "Epoch: 280, Loss: 0.0991, Accuracy: 0.9787, F1: 0.7612, Precision:  0.8226, Recall: 0.7083, ROC: 0.9188\n",
      "Epoch: 290, Loss: 0.0988, Accuracy: 0.9787, F1: 0.7612, Precision:  0.8226, Recall: 0.7083, ROC: 0.9187\n",
      "Epoch: 300, Loss: 0.0984, Accuracy: 0.9787, F1: 0.7612, Precision:  0.8226, Recall: 0.7083, ROC: 0.9184\n",
      "Epoch: 310, Loss: 0.0981, Accuracy: 0.9780, F1: 0.7568, Precision:  0.8063, Recall: 0.7130, ROC: 0.9178\n",
      "Epoch: 320, Loss: 0.0976, Accuracy: 0.9784, F1: 0.7651, Precision:  0.8020, Recall: 0.7315, ROC: 0.9150\n",
      "Epoch: 330, Loss: 0.0974, Accuracy: 0.9782, F1: 0.7621, Precision:  0.8010, Recall: 0.7269, ROC: 0.9146\n",
      "Epoch: 340, Loss: 0.0967, Accuracy: 0.9784, F1: 0.7617, Precision:  0.8115, Recall: 0.7176, ROC: 0.9157\n",
      "Epoch: 350, Loss: 0.0964, Accuracy: 0.9782, F1: 0.7598, Precision:  0.8073, Recall: 0.7176, ROC: 0.9156\n",
      "Epoch: 360, Loss: 0.0960, Accuracy: 0.9784, F1: 0.7617, Precision:  0.8115, Recall: 0.7176, ROC: 0.9154\n",
      "Epoch: 370, Loss: 0.0957, Accuracy: 0.9782, F1: 0.7598, Precision:  0.8073, Recall: 0.7176, ROC: 0.9151\n",
      "Epoch: 380, Loss: 0.0954, Accuracy: 0.9780, F1: 0.7579, Precision:  0.8031, Recall: 0.7176, ROC: 0.9151\n",
      "Epoch: 390, Loss: 0.0951, Accuracy: 0.9782, F1: 0.7598, Precision:  0.8073, Recall: 0.7176, ROC: 0.9149\n",
      "Epoch: 400, Loss: 0.0948, Accuracy: 0.9780, F1: 0.7579, Precision:  0.8031, Recall: 0.7176, ROC: 0.9148\n",
      "Epoch: 410, Loss: 0.0945, Accuracy: 0.9784, F1: 0.7640, Precision:  0.8051, Recall: 0.7269, ROC: 0.9140\n",
      "Epoch: 420, Loss: 0.0942, Accuracy: 0.9780, F1: 0.7603, Precision:  0.7970, Recall: 0.7269, ROC: 0.9134\n",
      "Epoch: 430, Loss: 0.0939, Accuracy: 0.9780, F1: 0.7603, Precision:  0.7970, Recall: 0.7269, ROC: 0.9134\n",
      "Epoch: 440, Loss: 0.0935, Accuracy: 0.9787, F1: 0.7692, Precision:  0.8000, Recall: 0.7407, ROC: 0.9133\n",
      "Epoch: 450, Loss: 0.0931, Accuracy: 0.9789, F1: 0.7711, Precision:  0.8040, Recall: 0.7407, ROC: 0.9134\n",
      "Epoch: 460, Loss: 0.0929, Accuracy: 0.9789, F1: 0.7711, Precision:  0.8040, Recall: 0.7407, ROC: 0.9134\n",
      "Epoch: 470, Loss: 0.0926, Accuracy: 0.9791, F1: 0.7718, Precision:  0.8112, Recall: 0.7361, ROC: 0.9136\n",
      "Epoch: 480, Loss: 0.0923, Accuracy: 0.9796, F1: 0.7778, Precision:  0.8131, Recall: 0.7454, ROC: 0.9140\n",
      "Epoch: 490, Loss: 0.0921, Accuracy: 0.9793, F1: 0.7748, Precision:  0.8122, Recall: 0.7407, ROC: 0.9144\n",
      "Epoch: 500, Loss: 0.0918, Accuracy: 0.9793, F1: 0.7737, Precision:  0.8154, Recall: 0.7361, ROC: 0.9145\n"
     ]
    }
   ],
   "source": [
    "num_features = 10\n",
    "num_classes = 2\n",
    "num_heads = 1\n",
    "hidden_dim = 128\n",
    "\n",
    "model = GAT(num_features, num_classes, num_heads, hidden_dim).to(device)\n",
    "\n",
    "num_epochs = 500\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    loss = train(model, data_graph_scaled, optimizer, scheduler)\n",
    "    acc, f1, precision, recall, roc = test(model, data_graph_scaled)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {precision: .4f}, Recall: {recall:.4f}, ROC: {roc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify fraud patterns in tfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_nodes = pd.read_csv('data/tfinance/selected_nodes.csv')\n",
    "selected_edges = pd.read_csv('data/tfinance/selected_edges.csv')\n",
    "selected_labels = pd.read_csv('data/tfinance/selected_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = list(pd.read_csv('data/tfinance/index/train_idx.csv', names=['id']).values.flatten())\n",
    "test_idx = list(pd.read_csv('data/tfinance/index/test_idx.csv', names=['id']).values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966892</td>\n",
       "      <td>-0.302345</td>\n",
       "      <td>-0.303793</td>\n",
       "      <td>-0.177117</td>\n",
       "      <td>-0.267849</td>\n",
       "      <td>-0.161530</td>\n",
       "      <td>-0.364640</td>\n",
       "      <td>-0.142817</td>\n",
       "      <td>-0.417713</td>\n",
       "      <td>-0.643477</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.975264</td>\n",
       "      <td>-0.182326</td>\n",
       "      <td>1.641039</td>\n",
       "      <td>0.465295</td>\n",
       "      <td>-0.268168</td>\n",
       "      <td>0.357262</td>\n",
       "      <td>-0.458247</td>\n",
       "      <td>0.386965</td>\n",
       "      <td>-0.894606</td>\n",
       "      <td>-0.848755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.541011</td>\n",
       "      <td>-0.278478</td>\n",
       "      <td>0.016163</td>\n",
       "      <td>-0.102169</td>\n",
       "      <td>-0.228462</td>\n",
       "      <td>-0.019719</td>\n",
       "      <td>0.033190</td>\n",
       "      <td>-0.022844</td>\n",
       "      <td>-0.875343</td>\n",
       "      <td>-0.872905</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.031149</td>\n",
       "      <td>-0.253015</td>\n",
       "      <td>-0.234783</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.134698</td>\n",
       "      <td>-0.192256</td>\n",
       "      <td>-0.516752</td>\n",
       "      <td>-0.160635</td>\n",
       "      <td>-0.226134</td>\n",
       "      <td>-0.688734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.275260</td>\n",
       "      <td>-0.281746</td>\n",
       "      <td>-0.278698</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>-0.209167</td>\n",
       "      <td>-0.160348</td>\n",
       "      <td>-0.271033</td>\n",
       "      <td>-0.152320</td>\n",
       "      <td>-0.409034</td>\n",
       "      <td>-0.404129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>-0.480986</td>\n",
       "      <td>-0.303485</td>\n",
       "      <td>-0.146952</td>\n",
       "      <td>-0.123583</td>\n",
       "      <td>-0.269444</td>\n",
       "      <td>-0.144986</td>\n",
       "      <td>-0.446546</td>\n",
       "      <td>-0.117872</td>\n",
       "      <td>0.729831</td>\n",
       "      <td>1.183965</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>-0.929844</td>\n",
       "      <td>0.330664</td>\n",
       "      <td>-0.297519</td>\n",
       "      <td>-0.252065</td>\n",
       "      <td>1.094280</td>\n",
       "      <td>-0.215891</td>\n",
       "      <td>-0.516752</td>\n",
       "      <td>-0.184392</td>\n",
       "      <td>-0.859675</td>\n",
       "      <td>-0.864036</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>-0.540210</td>\n",
       "      <td>-0.197832</td>\n",
       "      <td>0.185552</td>\n",
       "      <td>-0.102169</td>\n",
       "      <td>-0.268965</td>\n",
       "      <td>0.040551</td>\n",
       "      <td>-0.411444</td>\n",
       "      <td>0.072184</td>\n",
       "      <td>-0.205254</td>\n",
       "      <td>-0.243863</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>-0.845683</td>\n",
       "      <td>0.236032</td>\n",
       "      <td>-0.215962</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>-0.240421</td>\n",
       "      <td>-0.076443</td>\n",
       "      <td>-0.399743</td>\n",
       "      <td>-0.045413</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>0.183068</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>-0.367213</td>\n",
       "      <td>-0.244045</td>\n",
       "      <td>2.381330</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.253657</td>\n",
       "      <td>0.217815</td>\n",
       "      <td>-0.481649</td>\n",
       "      <td>0.250362</td>\n",
       "      <td>-0.575189</td>\n",
       "      <td>-0.778838</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0      0.966892 -0.302345 -0.303793 -0.177117 -0.267849 -0.161530 -0.364640   \n",
       "1      1.975264 -0.182326  1.641039  0.465295 -0.268168  0.357262 -0.458247   \n",
       "2      2.541011 -0.278478  0.016163 -0.102169 -0.228462 -0.019719  0.033190   \n",
       "3     -1.031149 -0.253015 -0.234783 -0.305600 -0.134698 -0.192256 -0.516752   \n",
       "4     -0.275260 -0.281746 -0.278698 -0.230652 -0.209167 -0.160348 -0.271033   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14995 -0.480986 -0.303485 -0.146952 -0.123583 -0.269444 -0.144986 -0.446546   \n",
       "14996 -0.929844  0.330664 -0.297519 -0.252065  1.094280 -0.215891 -0.516752   \n",
       "14997 -0.540210 -0.197832  0.185552 -0.102169 -0.268965  0.040551 -0.411444   \n",
       "14998 -0.845683  0.236032 -0.215962 -0.230652 -0.240421 -0.076443 -0.399743   \n",
       "14999 -0.367213 -0.244045  2.381330 -0.305600 -0.253657  0.217815 -0.481649   \n",
       "\n",
       "       feature8  feature9  feature10  label  \n",
       "0     -0.142817 -0.417713  -0.643477      0  \n",
       "1      0.386965 -0.894606  -0.848755      0  \n",
       "2     -0.022844 -0.875343  -0.872905      0  \n",
       "3     -0.160635 -0.226134  -0.688734      0  \n",
       "4     -0.152320 -0.409034  -0.404129      0  \n",
       "...         ...       ...        ...    ...  \n",
       "14995 -0.117872  0.729831   1.183965      0  \n",
       "14996 -0.184392 -0.859675  -0.864036      0  \n",
       "14997  0.072184 -0.205254  -0.243863      0  \n",
       "14998 -0.045413  0.264550   0.183068      0  \n",
       "14999  0.250362 -0.575189  -0.778838      0  \n",
       "\n",
       "[15000 rows x 11 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merge = pd.concat([selected_nodes_scaled, selected_labels], axis=1)\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the illicit ratio of a node's one hop neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The illicit ratio of 0's one hop neighbors is 0.00\n",
      "The illicit ratio of 1000's one hop neighbors is 0.04\n",
      "The illicit ratio of 2000's one hop neighbors is 0.01\n",
      "The illicit ratio of 3000's one hop neighbors is 0.00\n",
      "The illicit ratio of 4000's one hop neighbors is 0.00\n",
      "The illicit ratio of 5000's one hop neighbors is 0.01\n",
      "The illicit ratio of 6000's one hop neighbors is 0.17\n",
      "The illicit ratio of 7000's one hop neighbors is 0.00\n",
      "The illicit ratio of 8000's one hop neighbors is 0.01\n",
      "The illicit ratio of 9000's one hop neighbors is 0.04\n",
      "The illicit ratio of 10000's one hop neighbors is 0.01\n",
      "The illicit ratio of 11000's one hop neighbors is 0.00\n",
      "The illicit ratio of 12000's one hop neighbors is 0.04\n",
      "The illicit ratio of 13000's one hop neighbors is 0.79\n",
      "The illicit ratio of 14000's one hop neighbors is 0.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df_merge)):\n",
    "\n",
    "    selected_node = df_merge.index[i]\n",
    "    # filter the one hop edges\n",
    "    one_hop_edge = selected_edges[(selected_edges.ID1 == selected_node)|(selected_edges.ID2 == selected_node)]\n",
    "\n",
    "    # isolated node\n",
    "    if one_hop_edge.empty:\n",
    "        df_merge.at[i, 'illicit_ratio'] = 0\n",
    "    else:\n",
    "        # get the one hop neighbors\n",
    "        neighbors = list(set(one_hop_edge.ID1.values.tolist() + one_hop_edge.ID2.values.tolist()))\n",
    "        neighbors.remove(selected_node)\n",
    "\n",
    "        df_merge.at[i, 'degree'] = len(neighbors)\n",
    "\n",
    "        # value counts of the labels\n",
    "        value_counts = df_merge.loc[neighbors].label.value_counts()\n",
    "\n",
    "        if 1 not in value_counts.keys():\n",
    "            illicit_ratio = 0\n",
    "        else:\n",
    "            illicit_ratio = df_merge.loc[neighbors].label.value_counts(normalize=True)[1]\n",
    "        df_merge.at[i, 'illicit_ratio'] = illicit_ratio\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"The illicit ratio of {selected_node}'s one hop neighbors is {illicit_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_anomaly_graph(edge_df):\n",
    "\n",
    "    graph = nx.from_pandas_edgelist(edge_df, source='ID1', target='ID2')\n",
    "    pos = nx.spring_layout(graph)\n",
    "\n",
    "    class_labels = {}\n",
    "    for node in graph.nodes():\n",
    "        # label = res[res.txId == node]['label'].values[0]\n",
    "        label = df_merge[df_merge.index == node]['label'].values[0]\n",
    "        class_labels[node] = label\n",
    "    class_colors = {1: 'red', 0: 'blue', 2: 'green'}\n",
    "    node_colors = [class_colors[class_labels[node]] for node in graph.nodes()]\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw_networkx(graph, with_labels=False, pos=pos, node_color=node_colors)\n",
    "    # nx.draw_networkx(graph, with_labels=False, node_color=node_colors)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>label</th>\n",
       "      <th>degree</th>\n",
       "      <th>illicit_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10708</th>\n",
       "      <td>-0.766198</td>\n",
       "      <td>-0.321955</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.221800</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.189144</td>\n",
       "      <td>-1.114165</td>\n",
       "      <td>-0.898296</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3558</th>\n",
       "      <td>-1.018680</td>\n",
       "      <td>-0.218734</td>\n",
       "      <td>-0.178320</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.129754</td>\n",
       "      <td>-0.194619</td>\n",
       "      <td>-0.516752</td>\n",
       "      <td>-0.161823</td>\n",
       "      <td>1.602780</td>\n",
       "      <td>2.150146</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>-0.791134</td>\n",
       "      <td>-0.321955</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.221800</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.189144</td>\n",
       "      <td>-1.105276</td>\n",
       "      <td>-0.892393</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13657</th>\n",
       "      <td>-0.805161</td>\n",
       "      <td>-0.321727</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.273479</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.220618</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.187956</td>\n",
       "      <td>-1.142695</td>\n",
       "      <td>-0.888551</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11316</th>\n",
       "      <td>-0.773990</td>\n",
       "      <td>-0.321651</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.220618</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.187956</td>\n",
       "      <td>-1.149936</td>\n",
       "      <td>-0.896565</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4689</th>\n",
       "      <td>-0.381240</td>\n",
       "      <td>-0.005907</td>\n",
       "      <td>0.022437</td>\n",
       "      <td>0.047727</td>\n",
       "      <td>-0.256049</td>\n",
       "      <td>0.086639</td>\n",
       "      <td>-0.446546</td>\n",
       "      <td>0.118510</td>\n",
       "      <td>-0.858470</td>\n",
       "      <td>-0.767125</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10402</th>\n",
       "      <td>-0.284611</td>\n",
       "      <td>-0.314430</td>\n",
       "      <td>-0.297519</td>\n",
       "      <td>-0.284186</td>\n",
       "      <td>-0.265617</td>\n",
       "      <td>-0.152076</td>\n",
       "      <td>-0.341238</td>\n",
       "      <td>-0.134502</td>\n",
       "      <td>1.934668</td>\n",
       "      <td>2.401813</td>\n",
       "      <td>0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10401</th>\n",
       "      <td>-0.956339</td>\n",
       "      <td>-0.298772</td>\n",
       "      <td>-0.310066</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.260035</td>\n",
       "      <td>-0.212346</td>\n",
       "      <td>-0.493350</td>\n",
       "      <td>-0.179641</td>\n",
       "      <td>-0.819703</td>\n",
       "      <td>-0.776330</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10400</th>\n",
       "      <td>-0.678920</td>\n",
       "      <td>-0.305993</td>\n",
       "      <td>-0.159499</td>\n",
       "      <td>0.111968</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.134350</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.101242</td>\n",
       "      <td>-0.765683</td>\n",
       "      <td>-0.865633</td>\n",
       "      <td>0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966892</td>\n",
       "      <td>-0.302345</td>\n",
       "      <td>-0.303793</td>\n",
       "      <td>-0.177117</td>\n",
       "      <td>-0.267849</td>\n",
       "      <td>-0.161530</td>\n",
       "      <td>-0.364640</td>\n",
       "      <td>-0.142817</td>\n",
       "      <td>-0.417713</td>\n",
       "      <td>-0.643477</td>\n",
       "      <td>0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "10708 -0.766198 -0.321955 -0.316340 -0.294893 -0.273590 -0.221800 -0.528452   \n",
       "3558  -1.018680 -0.218734 -0.178320 -0.305600 -0.129754 -0.194619 -0.516752   \n",
       "4858  -0.791134 -0.321955 -0.316340 -0.294893 -0.273590 -0.221800 -0.528452   \n",
       "13657 -0.805161 -0.321727 -0.316340 -0.273479 -0.273590 -0.220618 -0.528452   \n",
       "11316 -0.773990 -0.321651 -0.316340 -0.294893 -0.273590 -0.220618 -0.528452   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "4689  -0.381240 -0.005907  0.022437  0.047727 -0.256049  0.086639 -0.446546   \n",
       "10402 -0.284611 -0.314430 -0.297519 -0.284186 -0.265617 -0.152076 -0.341238   \n",
       "10401 -0.956339 -0.298772 -0.310066 -0.294893 -0.260035 -0.212346 -0.493350   \n",
       "10400 -0.678920 -0.305993 -0.159499  0.111968 -0.273590 -0.134350 -0.528452   \n",
       "0      0.966892 -0.302345 -0.303793 -0.177117 -0.267849 -0.161530 -0.364640   \n",
       "\n",
       "       feature8  feature9  feature10  label  degree  illicit_ratio  \n",
       "10708 -0.189144 -1.114165  -0.898296      1    35.0            1.0  \n",
       "3558  -0.161823  1.602780   2.150146      0     2.0            1.0  \n",
       "4858  -0.189144 -1.105276  -0.892393      1    35.0            1.0  \n",
       "13657 -0.187956 -1.142695  -0.888551      1    35.0            1.0  \n",
       "11316 -0.187956 -1.149936  -0.896565      1    35.0            1.0  \n",
       "...         ...       ...        ...    ...     ...            ...  \n",
       "4689   0.118510 -0.858470  -0.767125      0     9.0            0.0  \n",
       "10402 -0.134502  1.934668   2.401813      0    17.0            0.0  \n",
       "10401 -0.179641 -0.819703  -0.776330      0     9.0            0.0  \n",
       "10400 -0.101242 -0.765683  -0.865633      0    63.0            0.0  \n",
       "0     -0.142817 -0.417713  -0.643477      0   124.0            0.0  \n",
       "\n",
       "[15000 rows x 13 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the dataframe by 'ratio' column in descending order\n",
    "df_ratio_sorted = df_merge.sort_values(by='illicit_ratio', ascending=False)\n",
    "df_ratio_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>label</th>\n",
       "      <th>degree</th>\n",
       "      <th>illicit_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>6.717882</td>\n",
       "      <td>1.833147</td>\n",
       "      <td>0.750181</td>\n",
       "      <td>1.514567</td>\n",
       "      <td>2.867344</td>\n",
       "      <td>0.943415</td>\n",
       "      <td>5.684716</td>\n",
       "      <td>0.549701</td>\n",
       "      <td>-0.156261</td>\n",
       "      <td>-0.311425</td>\n",
       "      <td>0</td>\n",
       "      <td>2822.0</td>\n",
       "      <td>0.010276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12560</th>\n",
       "      <td>2.667253</td>\n",
       "      <td>-0.238497</td>\n",
       "      <td>-0.034026</td>\n",
       "      <td>0.101261</td>\n",
       "      <td>-0.181899</td>\n",
       "      <td>0.015734</td>\n",
       "      <td>0.419319</td>\n",
       "      <td>-0.021656</td>\n",
       "      <td>-0.223256</td>\n",
       "      <td>-0.539823</td>\n",
       "      <td>0</td>\n",
       "      <td>2756.0</td>\n",
       "      <td>0.014877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2687</th>\n",
       "      <td>0.709734</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.053805</td>\n",
       "      <td>-0.144997</td>\n",
       "      <td>1.451636</td>\n",
       "      <td>0.046460</td>\n",
       "      <td>1.413894</td>\n",
       "      <td>-0.092927</td>\n",
       "      <td>-0.988842</td>\n",
       "      <td>-0.788853</td>\n",
       "      <td>1</td>\n",
       "      <td>2735.0</td>\n",
       "      <td>0.111517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9494</th>\n",
       "      <td>1.992407</td>\n",
       "      <td>0.168839</td>\n",
       "      <td>0.279657</td>\n",
       "      <td>0.186916</td>\n",
       "      <td>0.021575</td>\n",
       "      <td>0.588887</td>\n",
       "      <td>1.320287</td>\n",
       "      <td>0.508126</td>\n",
       "      <td>-0.745194</td>\n",
       "      <td>-0.785179</td>\n",
       "      <td>0</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>0.018315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6544</th>\n",
       "      <td>6.610344</td>\n",
       "      <td>0.643216</td>\n",
       "      <td>0.994853</td>\n",
       "      <td>1.386085</td>\n",
       "      <td>0.066703</td>\n",
       "      <td>0.893782</td>\n",
       "      <td>2.583982</td>\n",
       "      <td>0.767077</td>\n",
       "      <td>-0.252269</td>\n",
       "      <td>0.010416</td>\n",
       "      <td>0</td>\n",
       "      <td>2724.0</td>\n",
       "      <td>0.011013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13937</th>\n",
       "      <td>-0.956339</td>\n",
       "      <td>-0.266924</td>\n",
       "      <td>-0.303793</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.199759</td>\n",
       "      <td>-0.179257</td>\n",
       "      <td>-0.142323</td>\n",
       "      <td>-0.153508</td>\n",
       "      <td>1.894888</td>\n",
       "      <td>2.228186</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14203</th>\n",
       "      <td>-0.998419</td>\n",
       "      <td>-0.319751</td>\n",
       "      <td>-0.310066</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.270241</td>\n",
       "      <td>-0.208801</td>\n",
       "      <td>-0.411444</td>\n",
       "      <td>-0.182016</td>\n",
       "      <td>1.932876</td>\n",
       "      <td>2.515498</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14332</th>\n",
       "      <td>-1.031149</td>\n",
       "      <td>-0.264416</td>\n",
       "      <td>-0.303793</td>\n",
       "      <td>-0.091462</td>\n",
       "      <td>-0.243770</td>\n",
       "      <td>-0.187529</td>\n",
       "      <td>-0.423144</td>\n",
       "      <td>-0.154696</td>\n",
       "      <td>1.981945</td>\n",
       "      <td>1.483627</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14941</th>\n",
       "      <td>-1.009329</td>\n",
       "      <td>-0.295960</td>\n",
       "      <td>-0.310066</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.219213</td>\n",
       "      <td>-0.201710</td>\n",
       "      <td>-0.329537</td>\n",
       "      <td>-0.189144</td>\n",
       "      <td>2.227287</td>\n",
       "      <td>1.890944</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14961</th>\n",
       "      <td>-1.012446</td>\n",
       "      <td>-0.247390</td>\n",
       "      <td>-0.297519</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.201710</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.168950</td>\n",
       "      <td>2.227287</td>\n",
       "      <td>2.695136</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "168    6.717882  1.833147  0.750181  1.514567  2.867344  0.943415  5.684716   \n",
       "12560  2.667253 -0.238497 -0.034026  0.101261 -0.181899  0.015734  0.419319   \n",
       "2687   0.709734  0.592593  0.053805 -0.144997  1.451636  0.046460  1.413894   \n",
       "9494   1.992407  0.168839  0.279657  0.186916  0.021575  0.588887  1.320287   \n",
       "6544   6.610344  0.643216  0.994853  1.386085  0.066703  0.893782  2.583982   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "13937 -0.956339 -0.266924 -0.303793 -0.305600 -0.199759 -0.179257 -0.142323   \n",
       "14203 -0.998419 -0.319751 -0.310066 -0.305600 -0.270241 -0.208801 -0.411444   \n",
       "14332 -1.031149 -0.264416 -0.303793 -0.091462 -0.243770 -0.187529 -0.423144   \n",
       "14941 -1.009329 -0.295960 -0.310066 -0.294893 -0.219213 -0.201710 -0.329537   \n",
       "14961 -1.012446 -0.247390 -0.297519 -0.305600 -0.273590 -0.201710 -0.528452   \n",
       "\n",
       "       feature8  feature9  feature10  label  degree  illicit_ratio  \n",
       "168    0.549701 -0.156261  -0.311425      0  2822.0       0.010276  \n",
       "12560 -0.021656 -0.223256  -0.539823      0  2756.0       0.014877  \n",
       "2687  -0.092927 -0.988842  -0.788853      1  2735.0       0.111517  \n",
       "9494   0.508126 -0.745194  -0.785179      0  2730.0       0.018315  \n",
       "6544   0.767077 -0.252269   0.010416      0  2724.0       0.011013  \n",
       "...         ...       ...        ...    ...     ...            ...  \n",
       "13937 -0.153508  1.894888   2.228186      0     NaN       0.000000  \n",
       "14203 -0.182016  1.932876   2.515498      0     NaN       0.000000  \n",
       "14332 -0.154696  1.981945   1.483627      0     NaN       0.000000  \n",
       "14941 -0.189144  2.227287   1.890944      0     NaN       0.000000  \n",
       "14961 -0.168950  2.227287   2.695136      0     NaN       0.000000  \n",
       "\n",
       "[15000 rows x 13 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the sorted dataframe by 'degree' column in descending order\n",
    "df_degree_sorted = df_merge.sort_values(by='degree', ascending=False)\n",
    "df_degree_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_node = df_ratio_sorted.index[0]\n",
    "one_hop_edge = selected_edges[(selected_edges.ID1 == selected_node)|(selected_edges.ID2 == selected_node)]\n",
    "visualize_anomaly_graph(one_hop_edge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the top nodes from the training set, and also the edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort according to ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>label</th>\n",
       "      <th>degree</th>\n",
       "      <th>illicit_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13950</th>\n",
       "      <td>-0.830098</td>\n",
       "      <td>-0.321803</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.284186</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.220618</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.187956</td>\n",
       "      <td>-1.135443</td>\n",
       "      <td>-0.907110</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>-0.763081</td>\n",
       "      <td>-0.321955</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.221800</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.189144</td>\n",
       "      <td>-1.115169</td>\n",
       "      <td>-0.898963</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>-0.819188</td>\n",
       "      <td>-0.321727</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.284186</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.220618</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.187956</td>\n",
       "      <td>-1.138806</td>\n",
       "      <td>-0.909591</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11761</th>\n",
       "      <td>-0.725676</td>\n",
       "      <td>-0.321727</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.262772</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.220618</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.187956</td>\n",
       "      <td>-1.174790</td>\n",
       "      <td>-0.942176</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4302</th>\n",
       "      <td>-0.985951</td>\n",
       "      <td>-0.148349</td>\n",
       "      <td>-0.310066</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.091802</td>\n",
       "      <td>-0.207619</td>\n",
       "      <td>-0.411444</td>\n",
       "      <td>-0.183204</td>\n",
       "      <td>0.629711</td>\n",
       "      <td>0.365871</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>-0.844124</td>\n",
       "      <td>-0.313898</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.262772</td>\n",
       "      <td>-0.258600</td>\n",
       "      <td>-0.208801</td>\n",
       "      <td>-0.493350</td>\n",
       "      <td>-0.179641</td>\n",
       "      <td>0.432994</td>\n",
       "      <td>0.855425</td>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12532</th>\n",
       "      <td>-0.273701</td>\n",
       "      <td>-0.202088</td>\n",
       "      <td>-0.203414</td>\n",
       "      <td>-0.209238</td>\n",
       "      <td>-0.144106</td>\n",
       "      <td>-0.121350</td>\n",
       "      <td>-0.165725</td>\n",
       "      <td>-0.109557</td>\n",
       "      <td>-0.293848</td>\n",
       "      <td>-0.009872</td>\n",
       "      <td>0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1815</th>\n",
       "      <td>-0.861268</td>\n",
       "      <td>-0.282582</td>\n",
       "      <td>-0.291246</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.259398</td>\n",
       "      <td>-0.110715</td>\n",
       "      <td>-0.282734</td>\n",
       "      <td>-0.079861</td>\n",
       "      <td>0.244604</td>\n",
       "      <td>0.646914</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>-0.602552</td>\n",
       "      <td>-0.305537</td>\n",
       "      <td>-0.247330</td>\n",
       "      <td>-0.262772</td>\n",
       "      <td>-0.265457</td>\n",
       "      <td>-0.142622</td>\n",
       "      <td>-0.317837</td>\n",
       "      <td>-0.123812</td>\n",
       "      <td>1.942045</td>\n",
       "      <td>2.354638</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11797</th>\n",
       "      <td>-1.034266</td>\n",
       "      <td>-0.318003</td>\n",
       "      <td>-0.310066</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.218255</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.185580</td>\n",
       "      <td>-0.062572</td>\n",
       "      <td>0.306931</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10500 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "13950 -0.830098 -0.321803 -0.316340 -0.284186 -0.273590 -0.220618 -0.528452   \n",
       "675   -0.763081 -0.321955 -0.316340 -0.294893 -0.273590 -0.221800 -0.528452   \n",
       "972   -0.819188 -0.321727 -0.316340 -0.284186 -0.273590 -0.220618 -0.528452   \n",
       "11761 -0.725676 -0.321727 -0.316340 -0.262772 -0.273590 -0.220618 -0.528452   \n",
       "4302  -0.985951 -0.148349 -0.310066 -0.305600 -0.091802 -0.207619 -0.411444   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "211   -0.844124 -0.313898 -0.316340 -0.262772 -0.258600 -0.208801 -0.493350   \n",
       "12532 -0.273701 -0.202088 -0.203414 -0.209238 -0.144106 -0.121350 -0.165725   \n",
       "1815  -0.861268 -0.282582 -0.291246 -0.305600 -0.259398 -0.110715 -0.282734   \n",
       "3543  -0.602552 -0.305537 -0.247330 -0.262772 -0.265457 -0.142622 -0.317837   \n",
       "11797 -1.034266 -0.318003 -0.310066 -0.305600 -0.273590 -0.218255 -0.528452   \n",
       "\n",
       "       feature8  feature9  feature10  label  degree  illicit_ratio  \n",
       "13950 -0.187956 -1.135443  -0.907110      1    35.0            1.0  \n",
       "675   -0.189144 -1.115169  -0.898963      1    35.0            1.0  \n",
       "972   -0.187956 -1.138806  -0.909591      1    35.0            1.0  \n",
       "11761 -0.187956 -1.174790  -0.942176      1    35.0            1.0  \n",
       "4302  -0.183204  0.629711   0.365871      0     1.0            1.0  \n",
       "...         ...       ...        ...    ...     ...            ...  \n",
       "211   -0.179641  0.432994   0.855425      0     9.0            0.0  \n",
       "12532 -0.109557 -0.293848  -0.009872      0    77.0            0.0  \n",
       "1815  -0.079861  0.244604   0.646914      0     3.0            0.0  \n",
       "3543  -0.123812  1.942045   2.354638      0     3.0            0.0  \n",
       "11797 -0.185580 -0.062572   0.306931      0     4.0            0.0  \n",
       "\n",
       "[10500 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ratio_sorted = df_merge.loc[train_idx].sort_values(by='illicit_ratio', ascending=False)\n",
    "df_train_ratio_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort according to ratio and degree with a weighted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>label</th>\n",
       "      <th>degree</th>\n",
       "      <th>illicit_ratio</th>\n",
       "      <th>degree_norm</th>\n",
       "      <th>weighted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.966892</td>\n",
       "      <td>-0.302345</td>\n",
       "      <td>-0.303793</td>\n",
       "      <td>-0.177117</td>\n",
       "      <td>-0.267849</td>\n",
       "      <td>-0.161530</td>\n",
       "      <td>-0.364640</td>\n",
       "      <td>-0.142817</td>\n",
       "      <td>-0.417713</td>\n",
       "      <td>-0.643477</td>\n",
       "      <td>0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043602</td>\n",
       "      <td>0.004360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.975264</td>\n",
       "      <td>-0.182326</td>\n",
       "      <td>1.641039</td>\n",
       "      <td>0.465295</td>\n",
       "      <td>-0.268168</td>\n",
       "      <td>0.357262</td>\n",
       "      <td>-0.458247</td>\n",
       "      <td>0.386965</td>\n",
       "      <td>-0.894606</td>\n",
       "      <td>-0.848755</td>\n",
       "      <td>0</td>\n",
       "      <td>2197.0</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>0.778447</td>\n",
       "      <td>0.086447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.541011</td>\n",
       "      <td>-0.278478</td>\n",
       "      <td>0.016163</td>\n",
       "      <td>-0.102169</td>\n",
       "      <td>-0.228462</td>\n",
       "      <td>-0.019719</td>\n",
       "      <td>0.033190</td>\n",
       "      <td>-0.022844</td>\n",
       "      <td>-0.875343</td>\n",
       "      <td>-0.872905</td>\n",
       "      <td>0</td>\n",
       "      <td>2121.0</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.751507</td>\n",
       "      <td>0.083637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.031149</td>\n",
       "      <td>-0.253015</td>\n",
       "      <td>-0.234783</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.134698</td>\n",
       "      <td>-0.192256</td>\n",
       "      <td>-0.516752</td>\n",
       "      <td>-0.160635</td>\n",
       "      <td>-0.226134</td>\n",
       "      <td>-0.688734</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003190</td>\n",
       "      <td>0.000319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.275260</td>\n",
       "      <td>-0.281746</td>\n",
       "      <td>-0.278698</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>-0.209167</td>\n",
       "      <td>-0.160348</td>\n",
       "      <td>-0.271033</td>\n",
       "      <td>-0.152320</td>\n",
       "      <td>-0.409034</td>\n",
       "      <td>-0.404129</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018079</td>\n",
       "      <td>0.001808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>-0.480986</td>\n",
       "      <td>-0.303485</td>\n",
       "      <td>-0.146952</td>\n",
       "      <td>-0.123583</td>\n",
       "      <td>-0.269444</td>\n",
       "      <td>-0.144986</td>\n",
       "      <td>-0.446546</td>\n",
       "      <td>-0.117872</td>\n",
       "      <td>0.729831</td>\n",
       "      <td>1.183965</td>\n",
       "      <td>0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.030840</td>\n",
       "      <td>0.023539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>-0.929844</td>\n",
       "      <td>0.330664</td>\n",
       "      <td>-0.297519</td>\n",
       "      <td>-0.252065</td>\n",
       "      <td>1.094280</td>\n",
       "      <td>-0.215891</td>\n",
       "      <td>-0.516752</td>\n",
       "      <td>-0.184392</td>\n",
       "      <td>-0.859675</td>\n",
       "      <td>-0.864036</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>-0.540210</td>\n",
       "      <td>-0.197832</td>\n",
       "      <td>0.185552</td>\n",
       "      <td>-0.102169</td>\n",
       "      <td>-0.268965</td>\n",
       "      <td>0.040551</td>\n",
       "      <td>-0.411444</td>\n",
       "      <td>0.072184</td>\n",
       "      <td>-0.205254</td>\n",
       "      <td>-0.243863</td>\n",
       "      <td>0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.001099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>-0.845683</td>\n",
       "      <td>0.236032</td>\n",
       "      <td>-0.215962</td>\n",
       "      <td>-0.230652</td>\n",
       "      <td>-0.240421</td>\n",
       "      <td>-0.076443</td>\n",
       "      <td>-0.399743</td>\n",
       "      <td>-0.045413</td>\n",
       "      <td>0.264550</td>\n",
       "      <td>0.183068</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.001028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>-0.367213</td>\n",
       "      <td>-0.244045</td>\n",
       "      <td>2.381330</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.253657</td>\n",
       "      <td>0.217815</td>\n",
       "      <td>-0.481649</td>\n",
       "      <td>0.250362</td>\n",
       "      <td>-0.575189</td>\n",
       "      <td>-0.778838</td>\n",
       "      <td>0</td>\n",
       "      <td>2204.0</td>\n",
       "      <td>0.009528</td>\n",
       "      <td>0.780929</td>\n",
       "      <td>0.086668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0      0.966892 -0.302345 -0.303793 -0.177117 -0.267849 -0.161530 -0.364640   \n",
       "1      1.975264 -0.182326  1.641039  0.465295 -0.268168  0.357262 -0.458247   \n",
       "2      2.541011 -0.278478  0.016163 -0.102169 -0.228462 -0.019719  0.033190   \n",
       "3     -1.031149 -0.253015 -0.234783 -0.305600 -0.134698 -0.192256 -0.516752   \n",
       "4     -0.275260 -0.281746 -0.278698 -0.230652 -0.209167 -0.160348 -0.271033   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "14995 -0.480986 -0.303485 -0.146952 -0.123583 -0.269444 -0.144986 -0.446546   \n",
       "14996 -0.929844  0.330664 -0.297519 -0.252065  1.094280 -0.215891 -0.516752   \n",
       "14997 -0.540210 -0.197832  0.185552 -0.102169 -0.268965  0.040551 -0.411444   \n",
       "14998 -0.845683  0.236032 -0.215962 -0.230652 -0.240421 -0.076443 -0.399743   \n",
       "14999 -0.367213 -0.244045  2.381330 -0.305600 -0.253657  0.217815 -0.481649   \n",
       "\n",
       "       feature8  feature9  feature10  label  degree  illicit_ratio  \\\n",
       "0     -0.142817 -0.417713  -0.643477      0   124.0       0.000000   \n",
       "1      0.386965 -0.894606  -0.848755      0  2197.0       0.009558   \n",
       "2     -0.022844 -0.875343  -0.872905      0  2121.0       0.009430   \n",
       "3     -0.160635 -0.226134  -0.688734      0    10.0       0.000000   \n",
       "4     -0.152320 -0.409034  -0.404129      0    52.0       0.000000   \n",
       "...         ...       ...        ...    ...     ...            ...   \n",
       "14995 -0.117872  0.729831   1.183965      0    88.0       0.022727   \n",
       "14996 -0.184392 -0.859675  -0.864036      0     3.0       0.000000   \n",
       "14997  0.072184 -0.205254  -0.243863      0    32.0       0.000000   \n",
       "14998 -0.045413  0.264550   0.183068      0    30.0       0.000000   \n",
       "14999  0.250362 -0.575189  -0.778838      0  2204.0       0.009528   \n",
       "\n",
       "       degree_norm  weighted_score  \n",
       "0         0.043602        0.004360  \n",
       "1         0.778447        0.086447  \n",
       "2         0.751507        0.083637  \n",
       "3         0.003190        0.000319  \n",
       "4         0.018079        0.001808  \n",
       "...            ...             ...  \n",
       "14995     0.030840        0.023539  \n",
       "14996     0.000709        0.000071  \n",
       "14997     0.010989        0.001099  \n",
       "14998     0.010280        0.001028  \n",
       "14999     0.780929        0.086668  \n",
       "\n",
       "[15000 rows x 15 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_weight  = 0.9\n",
    "degree_weight = 0.1\n",
    "df_merge['degree_norm'] = normalized_column = (df_merge['degree'] - df_merge['degree'].min()) / (df_merge['degree'].max() - df_merge['degree'].min())\n",
    "df_merge['weighted_score'] = df_merge['illicit_ratio'] * ratio_weight + df_merge['degree_norm'] * degree_weight\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>feature10</th>\n",
       "      <th>label</th>\n",
       "      <th>degree</th>\n",
       "      <th>illicit_ratio</th>\n",
       "      <th>degree_norm</th>\n",
       "      <th>weighted_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>-0.791134</td>\n",
       "      <td>-0.321955</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.221800</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.189144</td>\n",
       "      <td>-1.105276</td>\n",
       "      <td>-0.892393</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012052</td>\n",
       "      <td>0.901205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>-0.819188</td>\n",
       "      <td>-0.321727</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.284186</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.220618</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.187956</td>\n",
       "      <td>-1.138806</td>\n",
       "      <td>-0.909591</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012052</td>\n",
       "      <td>0.901205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2577</th>\n",
       "      <td>-0.780225</td>\n",
       "      <td>-0.321727</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.273479</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.220618</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.187956</td>\n",
       "      <td>-1.168247</td>\n",
       "      <td>-0.916832</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012052</td>\n",
       "      <td>0.901205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912</th>\n",
       "      <td>-0.778666</td>\n",
       "      <td>-0.321575</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.273479</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.219436</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.186768</td>\n",
       "      <td>-1.148954</td>\n",
       "      <td>-0.895479</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012052</td>\n",
       "      <td>0.901205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11870</th>\n",
       "      <td>-0.769315</td>\n",
       "      <td>-0.321955</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.273590</td>\n",
       "      <td>-0.221800</td>\n",
       "      <td>-0.528452</td>\n",
       "      <td>-0.189144</td>\n",
       "      <td>-1.113140</td>\n",
       "      <td>-0.897615</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012052</td>\n",
       "      <td>0.901205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5971</th>\n",
       "      <td>-0.918934</td>\n",
       "      <td>-0.011684</td>\n",
       "      <td>-0.241056</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.273271</td>\n",
       "      <td>-0.144986</td>\n",
       "      <td>-0.516752</td>\n",
       "      <td>-0.111933</td>\n",
       "      <td>1.748014</td>\n",
       "      <td>2.178277</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12536</th>\n",
       "      <td>-1.003095</td>\n",
       "      <td>-0.277642</td>\n",
       "      <td>-0.291246</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.197367</td>\n",
       "      <td>-0.202892</td>\n",
       "      <td>-0.376341</td>\n",
       "      <td>-0.178453</td>\n",
       "      <td>1.475927</td>\n",
       "      <td>1.415740</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>-0.943871</td>\n",
       "      <td>12.651604</td>\n",
       "      <td>0.122816</td>\n",
       "      <td>-0.305600</td>\n",
       "      <td>-0.272792</td>\n",
       "      <td>-0.113078</td>\n",
       "      <td>-0.481649</td>\n",
       "      <td>-0.079861</td>\n",
       "      <td>-0.373339</td>\n",
       "      <td>-0.417189</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12289</th>\n",
       "      <td>-1.029590</td>\n",
       "      <td>-0.297480</td>\n",
       "      <td>-0.316340</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.232767</td>\n",
       "      <td>-0.208801</td>\n",
       "      <td>-0.411444</td>\n",
       "      <td>-0.180829</td>\n",
       "      <td>1.769315</td>\n",
       "      <td>2.334468</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13067</th>\n",
       "      <td>-0.964132</td>\n",
       "      <td>-0.195399</td>\n",
       "      <td>-0.278698</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.019087</td>\n",
       "      <td>-0.170984</td>\n",
       "      <td>-0.072118</td>\n",
       "      <td>-0.167762</td>\n",
       "      <td>0.901579</td>\n",
       "      <td>1.374056</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10500 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature1   feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "4858  -0.791134  -0.321955 -0.316340 -0.294893 -0.273590 -0.221800 -0.528452   \n",
       "972   -0.819188  -0.321727 -0.316340 -0.284186 -0.273590 -0.220618 -0.528452   \n",
       "2577  -0.780225  -0.321727 -0.316340 -0.273479 -0.273590 -0.220618 -0.528452   \n",
       "8912  -0.778666  -0.321575 -0.316340 -0.273479 -0.273590 -0.219436 -0.528452   \n",
       "11870 -0.769315  -0.321955 -0.316340 -0.294893 -0.273590 -0.221800 -0.528452   \n",
       "...         ...        ...       ...       ...       ...       ...       ...   \n",
       "5971  -0.918934  -0.011684 -0.241056 -0.294893 -0.273271 -0.144986 -0.516752   \n",
       "12536 -1.003095  -0.277642 -0.291246 -0.305600 -0.197367 -0.202892 -0.376341   \n",
       "2923  -0.943871  12.651604  0.122816 -0.305600 -0.272792 -0.113078 -0.481649   \n",
       "12289 -1.029590  -0.297480 -0.316340 -0.294893 -0.232767 -0.208801 -0.411444   \n",
       "13067 -0.964132  -0.195399 -0.278698 -0.294893 -0.019087 -0.170984 -0.072118   \n",
       "\n",
       "       feature8  feature9  feature10  label  degree  illicit_ratio  \\\n",
       "4858  -0.189144 -1.105276  -0.892393      1    35.0            1.0   \n",
       "972   -0.187956 -1.138806  -0.909591      1    35.0            1.0   \n",
       "2577  -0.187956 -1.168247  -0.916832      1    35.0            1.0   \n",
       "8912  -0.186768 -1.148954  -0.895479      1    35.0            1.0   \n",
       "11870 -0.189144 -1.113140  -0.897615      1    35.0            1.0   \n",
       "...         ...       ...        ...    ...     ...            ...   \n",
       "5971  -0.111933  1.748014   2.178277      0     NaN            0.0   \n",
       "12536 -0.178453  1.475927   1.415740      0     NaN            0.0   \n",
       "2923  -0.079861 -0.373339  -0.417189      0     NaN            0.0   \n",
       "12289 -0.180829  1.769315   2.334468      0     NaN            0.0   \n",
       "13067 -0.167762  0.901579   1.374056      0     NaN            0.0   \n",
       "\n",
       "       degree_norm  weighted_score  \n",
       "4858      0.012052        0.901205  \n",
       "972       0.012052        0.901205  \n",
       "2577      0.012052        0.901205  \n",
       "8912      0.012052        0.901205  \n",
       "11870     0.012052        0.901205  \n",
       "...            ...             ...  \n",
       "5971           NaN             NaN  \n",
       "12536          NaN             NaN  \n",
       "2923           NaN             NaN  \n",
       "12289          NaN             NaN  \n",
       "13067          NaN             NaN  \n",
       "\n",
       "[10500 rows x 15 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_weight_sorted = df_merge.loc[train_idx].sort_values(by='weighted_score', ascending=False)\n",
    "df_train_weight_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_to_remove = df_train_weight_sorted.index[:50].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges before mask:  6032438\n",
      "Number of edges after  mask:  6026522\n"
     ]
    }
   ],
   "source": [
    "print('Number of edges before mask: ', len(selected_edges))\n",
    "selected_edges_filtered = selected_edges[~selected_edges['ID1'].isin(nodes_to_remove) & ~selected_edges['ID2'].isin(nodes_to_remove)]\n",
    "print('Number of edges after  mask: ', len(selected_edges_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12230, 12230, 12230,  ..., 10945, 10945, 10945],\n",
      "        [ 6579,  8455,   272,  ...,  8153,  6480,  9753]])\n",
      "shape of edge index filtered is torch.Size([2, 6026522])\n"
     ]
    }
   ],
   "source": [
    "edge_index_filtered = np.array(selected_edges_filtered.values).T \n",
    "edge_index_filtered = torch.tensor(edge_index_filtered, dtype=torch.long)\n",
    "print(edge_index_filtered)\n",
    "\n",
    "print(\"shape of edge index filtered is {}\".format(edge_index_filtered.shape))\n",
    "\n",
    "node_features_scaled_filtered_t = torch.tensor(np.array(selected_nodes_scaled.values, dtype=np.double), dtype=torch.double)\n",
    "data_graph_scaled_filtered = Data(x=node_features_scaled_filtered_t.float(), edge_index=edge_index_filtered,\n",
    "                         y=torch.tensor(selected_labels.values.flatten(), dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14284\n",
       "1      716\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_labels.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trains before mask:  tensor(10500)\n",
      "Number of trains after mask:  tensor(10450)\n"
     ]
    }
   ],
   "source": [
    "sample_size = 15000\n",
    "\n",
    "data_graph_scaled_filtered.train_idx = torch.zeros(sample_size, dtype=torch.bool)\n",
    "data_graph_scaled_filtered.train_idx[train_idx] = 1\n",
    "print('Number of trains before mask: ', data_graph_scaled_filtered.train_idx.sum())\n",
    "data_graph_scaled_filtered.train_idx[nodes_to_remove] = 0\n",
    "print('Number of trains after mask: ', data_graph_scaled_filtered.train_idx.sum())\n",
    "\n",
    "data_graph_scaled_filtered.test_idx = torch.zeros(sample_size, dtype=torch.bool)\n",
    "data_graph_scaled_filtered.test_idx[test_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train GCN with top nodes masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.7336, Accuracy: 0.8962, F1: 0.0251, Precision:  0.0228, Recall: 0.0278, ROC: 0.6637\n",
      "Epoch: 010, Loss: 0.1299, Accuracy: 0.9733, F1: 0.6739, Precision:  0.8158, Recall: 0.5741, ROC: 0.9094\n",
      "Epoch: 020, Loss: 0.1224, Accuracy: 0.9747, F1: 0.7121, Precision:  0.7833, Recall: 0.6528, ROC: 0.9149\n",
      "Epoch: 030, Loss: 0.1147, Accuracy: 0.9764, F1: 0.7240, Precision:  0.8274, Recall: 0.6435, ROC: 0.9127\n",
      "Epoch: 040, Loss: 0.1119, Accuracy: 0.9764, F1: 0.7166, Precision:  0.8481, Recall: 0.6204, ROC: 0.9135\n",
      "Epoch: 050, Loss: 0.1095, Accuracy: 0.9762, F1: 0.7116, Precision:  0.8516, Recall: 0.6111, ROC: 0.9152\n",
      "Epoch: 060, Loss: 0.1088, Accuracy: 0.9764, F1: 0.7151, Precision:  0.8526, Recall: 0.6157, ROC: 0.9154\n",
      "Epoch: 070, Loss: 0.1082, Accuracy: 0.9769, F1: 0.7234, Precision:  0.8500, Recall: 0.6296, ROC: 0.9155\n",
      "Epoch: 080, Loss: 0.1074, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9156\n",
      "Epoch: 090, Loss: 0.1068, Accuracy: 0.9769, F1: 0.7249, Precision:  0.8457, Recall: 0.6343, ROC: 0.9156\n",
      "Epoch: 100, Loss: 0.1062, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9157\n",
      "Epoch: 110, Loss: 0.1056, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9158\n",
      "Epoch: 120, Loss: 0.1051, Accuracy: 0.9782, F1: 0.7421, Precision:  0.8598, Recall: 0.6528, ROC: 0.9158\n",
      "Epoch: 130, Loss: 0.1045, Accuracy: 0.9787, F1: 0.7474, Precision:  0.8659, Recall: 0.6574, ROC: 0.9158\n",
      "Epoch: 140, Loss: 0.1040, Accuracy: 0.9789, F1: 0.7493, Precision:  0.8712, Recall: 0.6574, ROC: 0.9158\n",
      "Epoch: 150, Loss: 0.1036, Accuracy: 0.9791, F1: 0.7526, Precision:  0.8720, Recall: 0.6620, ROC: 0.9159\n",
      "Epoch: 160, Loss: 0.1032, Accuracy: 0.9789, F1: 0.7493, Precision:  0.8712, Recall: 0.6574, ROC: 0.9161\n",
      "Epoch: 170, Loss: 0.1028, Accuracy: 0.9789, F1: 0.7493, Precision:  0.8712, Recall: 0.6574, ROC: 0.9162\n",
      "Epoch: 180, Loss: 0.1025, Accuracy: 0.9789, F1: 0.7493, Precision:  0.8712, Recall: 0.6574, ROC: 0.9163\n",
      "Epoch: 190, Loss: 0.1023, Accuracy: 0.9787, F1: 0.7460, Precision:  0.8704, Recall: 0.6528, ROC: 0.9165\n",
      "Epoch: 200, Loss: 0.1021, Accuracy: 0.9782, F1: 0.7407, Precision:  0.8642, Recall: 0.6481, ROC: 0.9169\n",
      "Epoch: 210, Loss: 0.1019, Accuracy: 0.9780, F1: 0.7388, Precision:  0.8589, Recall: 0.6481, ROC: 0.9172\n",
      "Epoch: 220, Loss: 0.1017, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9175\n",
      "Epoch: 230, Loss: 0.1016, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9178\n",
      "Epoch: 240, Loss: 0.1014, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9181\n",
      "Epoch: 250, Loss: 0.1013, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9184\n",
      "Epoch: 260, Loss: 0.1012, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9185\n",
      "Epoch: 270, Loss: 0.1011, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9187\n",
      "Epoch: 280, Loss: 0.1009, Accuracy: 0.9771, F1: 0.7282, Precision:  0.8466, Recall: 0.6389, ROC: 0.9188\n",
      "Epoch: 290, Loss: 0.1008, Accuracy: 0.9769, F1: 0.7249, Precision:  0.8457, Recall: 0.6343, ROC: 0.9189\n",
      "Epoch: 300, Loss: 0.1007, Accuracy: 0.9769, F1: 0.7249, Precision:  0.8457, Recall: 0.6343, ROC: 0.9190\n",
      "Epoch: 310, Loss: 0.1006, Accuracy: 0.9769, F1: 0.7249, Precision:  0.8457, Recall: 0.6343, ROC: 0.9190\n",
      "Epoch: 320, Loss: 0.1005, Accuracy: 0.9769, F1: 0.7249, Precision:  0.8457, Recall: 0.6343, ROC: 0.9191\n",
      "Epoch: 330, Loss: 0.1004, Accuracy: 0.9769, F1: 0.7249, Precision:  0.8457, Recall: 0.6343, ROC: 0.9191\n",
      "Epoch: 340, Loss: 0.1003, Accuracy: 0.9769, F1: 0.7249, Precision:  0.8457, Recall: 0.6343, ROC: 0.9191\n",
      "Epoch: 350, Loss: 0.1002, Accuracy: 0.9769, F1: 0.7249, Precision:  0.8457, Recall: 0.6343, ROC: 0.9191\n",
      "Epoch: 360, Loss: 0.1001, Accuracy: 0.9769, F1: 0.7249, Precision:  0.8457, Recall: 0.6343, ROC: 0.9191\n",
      "Epoch: 370, Loss: 0.1000, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9191\n",
      "Epoch: 380, Loss: 0.0998, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9192\n",
      "Epoch: 390, Loss: 0.0997, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9191\n",
      "Epoch: 400, Loss: 0.0996, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9192\n",
      "Epoch: 410, Loss: 0.0995, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9192\n",
      "Epoch: 420, Loss: 0.0994, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9192\n",
      "Epoch: 430, Loss: 0.0993, Accuracy: 0.9773, F1: 0.7302, Precision:  0.8519, Recall: 0.6389, ROC: 0.9193\n",
      "Epoch: 440, Loss: 0.0992, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9193\n",
      "Epoch: 450, Loss: 0.0991, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9193\n",
      "Epoch: 460, Loss: 0.0990, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9194\n",
      "Epoch: 470, Loss: 0.0989, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9194\n",
      "Epoch: 480, Loss: 0.0988, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9195\n",
      "Epoch: 490, Loss: 0.0986, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9195\n",
      "Epoch: 500, Loss: 0.0985, Accuracy: 0.9778, F1: 0.7354, Precision:  0.8580, Recall: 0.6435, ROC: 0.9196\n"
     ]
    }
   ],
   "source": [
    "model = GCN(num_features, 2, hidden_channels=256).to(device)\n",
    "num_epochs = 500\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    loss = train(model, data_graph_scaled_filtered, optimizer, scheduler)\n",
    "    acc, f1, precision, recall, roc = test(model, data_graph_scaled_filtered)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {precision: .4f}, Recall: {recall:.4f}, ROC: {roc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train GAT with top nodes masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(453, device='cuda:0')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.count_nonzero(data_graph_scaled_filtered.y[data_graph_scaled_filtered.train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.6226, Accuracy: 0.9460, F1: 0.1763, Precision:  0.3291, Recall: 0.1204, ROC: 0.6606\n",
      "Epoch: 010, Loss: 0.1770, Accuracy: 0.9496, F1: 0.0340, Precision:  0.2105, Recall: 0.0185, ROC: 0.8019\n",
      "Epoch: 020, Loss: 0.1626, Accuracy: 0.9518, F1: 0.0269, Precision:  0.4286, Recall: 0.0139, ROC: 0.7923\n",
      "Epoch: 030, Loss: 0.1450, Accuracy: 0.9627, F1: 0.5030, Precision:  0.6967, Recall: 0.3935, ROC: 0.8565\n",
      "Epoch: 040, Loss: 0.1311, Accuracy: 0.9647, F1: 0.5714, Precision:  0.6839, Recall: 0.4907, ROC: 0.8626\n",
      "Epoch: 050, Loss: 0.1236, Accuracy: 0.9689, F1: 0.6216, Precision:  0.7468, Recall: 0.5324, ROC: 0.8698\n",
      "Epoch: 060, Loss: 0.1155, Accuracy: 0.9722, F1: 0.6499, Precision:  0.8227, Recall: 0.5370, ROC: 0.8805\n",
      "Epoch: 070, Loss: 0.1114, Accuracy: 0.9744, F1: 0.6866, Precision:  0.8344, Recall: 0.5833, ROC: 0.8923\n",
      "Epoch: 080, Loss: 0.1088, Accuracy: 0.9762, F1: 0.7068, Precision:  0.8658, Recall: 0.5972, ROC: 0.8985\n",
      "Epoch: 090, Loss: 0.1069, Accuracy: 0.9771, F1: 0.7209, Precision:  0.8693, Recall: 0.6157, ROC: 0.9003\n",
      "Epoch: 100, Loss: 0.1058, Accuracy: 0.9771, F1: 0.7239, Precision:  0.8599, Recall: 0.6250, ROC: 0.9008\n",
      "Epoch: 110, Loss: 0.1049, Accuracy: 0.9773, F1: 0.7273, Precision:  0.8608, Recall: 0.6296, ROC: 0.9012\n",
      "Epoch: 120, Loss: 0.1042, Accuracy: 0.9778, F1: 0.7326, Precision:  0.8671, Recall: 0.6343, ROC: 0.9020\n",
      "Epoch: 130, Loss: 0.1035, Accuracy: 0.9778, F1: 0.7312, Precision:  0.8718, Recall: 0.6296, ROC: 0.9035\n",
      "Epoch: 140, Loss: 0.1029, Accuracy: 0.9778, F1: 0.7326, Precision:  0.8671, Recall: 0.6343, ROC: 0.9043\n",
      "Epoch: 150, Loss: 0.1022, Accuracy: 0.9780, F1: 0.7346, Precision:  0.8726, Recall: 0.6343, ROC: 0.9052\n",
      "Epoch: 160, Loss: 0.1015, Accuracy: 0.9778, F1: 0.7326, Precision:  0.8671, Recall: 0.6343, ROC: 0.9061\n",
      "Epoch: 170, Loss: 0.1009, Accuracy: 0.9778, F1: 0.7340, Precision:  0.8625, Recall: 0.6389, ROC: 0.9065\n",
      "Epoch: 180, Loss: 0.1004, Accuracy: 0.9776, F1: 0.7321, Precision:  0.8571, Recall: 0.6389, ROC: 0.9074\n",
      "Epoch: 190, Loss: 0.0998, Accuracy: 0.9776, F1: 0.7321, Precision:  0.8571, Recall: 0.6389, ROC: 0.9082\n",
      "Epoch: 200, Loss: 0.0993, Accuracy: 0.9778, F1: 0.7354, Precision:  0.8580, Recall: 0.6435, ROC: 0.9090\n",
      "Epoch: 210, Loss: 0.0988, Accuracy: 0.9778, F1: 0.7354, Precision:  0.8580, Recall: 0.6435, ROC: 0.9100\n",
      "Epoch: 220, Loss: 0.0983, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9108\n",
      "Epoch: 230, Loss: 0.0978, Accuracy: 0.9778, F1: 0.7354, Precision:  0.8580, Recall: 0.6435, ROC: 0.9113\n",
      "Epoch: 240, Loss: 0.0973, Accuracy: 0.9778, F1: 0.7354, Precision:  0.8580, Recall: 0.6435, ROC: 0.9118\n",
      "Epoch: 250, Loss: 0.0969, Accuracy: 0.9780, F1: 0.7374, Precision:  0.8634, Recall: 0.6435, ROC: 0.9124\n",
      "Epoch: 260, Loss: 0.0964, Accuracy: 0.9780, F1: 0.7374, Precision:  0.8634, Recall: 0.6435, ROC: 0.9132\n",
      "Epoch: 270, Loss: 0.0959, Accuracy: 0.9778, F1: 0.7354, Precision:  0.8580, Recall: 0.6435, ROC: 0.9140\n",
      "Epoch: 280, Loss: 0.0954, Accuracy: 0.9776, F1: 0.7335, Precision:  0.8528, Recall: 0.6435, ROC: 0.9145\n",
      "Epoch: 290, Loss: 0.0948, Accuracy: 0.9773, F1: 0.7316, Precision:  0.8476, Recall: 0.6435, ROC: 0.9149\n",
      "Epoch: 300, Loss: 0.0942, Accuracy: 0.9773, F1: 0.7330, Precision:  0.8434, Recall: 0.6481, ROC: 0.9154\n",
      "Epoch: 310, Loss: 0.0935, Accuracy: 0.9776, F1: 0.7363, Precision:  0.8443, Recall: 0.6528, ROC: 0.9162\n",
      "Epoch: 320, Loss: 0.0927, Accuracy: 0.9776, F1: 0.7377, Precision:  0.8402, Recall: 0.6574, ROC: 0.9144\n",
      "Epoch: 330, Loss: 0.0921, Accuracy: 0.9776, F1: 0.7377, Precision:  0.8402, Recall: 0.6574, ROC: 0.9158\n",
      "Epoch: 340, Loss: 0.0916, Accuracy: 0.9784, F1: 0.7454, Precision:  0.8606, Recall: 0.6574, ROC: 0.9163\n",
      "Epoch: 350, Loss: 0.0911, Accuracy: 0.9789, F1: 0.7507, Precision:  0.8667, Recall: 0.6620, ROC: 0.9166\n",
      "Epoch: 360, Loss: 0.0907, Accuracy: 0.9793, F1: 0.7559, Precision:  0.8727, Recall: 0.6667, ROC: 0.9171\n",
      "Epoch: 370, Loss: 0.0903, Accuracy: 0.9793, F1: 0.7559, Precision:  0.8727, Recall: 0.6667, ROC: 0.9177\n",
      "Epoch: 380, Loss: 0.0899, Accuracy: 0.9793, F1: 0.7559, Precision:  0.8727, Recall: 0.6667, ROC: 0.9183\n",
      "Epoch: 390, Loss: 0.0894, Accuracy: 0.9793, F1: 0.7559, Precision:  0.8727, Recall: 0.6667, ROC: 0.9192\n",
      "Epoch: 400, Loss: 0.0888, Accuracy: 0.9796, F1: 0.7592, Precision:  0.8735, Recall: 0.6713, ROC: 0.9212\n",
      "Epoch: 410, Loss: 0.0882, Accuracy: 0.9796, F1: 0.7579, Precision:  0.8780, Recall: 0.6667, ROC: 0.9229\n",
      "Epoch: 420, Loss: 0.0877, Accuracy: 0.9796, F1: 0.7592, Precision:  0.8735, Recall: 0.6713, ROC: 0.9241\n",
      "Epoch: 430, Loss: 0.0872, Accuracy: 0.9800, F1: 0.7656, Precision:  0.8750, Recall: 0.6806, ROC: 0.9251\n",
      "Epoch: 440, Loss: 0.0866, Accuracy: 0.9798, F1: 0.7636, Precision:  0.8698, Recall: 0.6806, ROC: 0.9252\n",
      "Epoch: 450, Loss: 0.0862, Accuracy: 0.9798, F1: 0.7636, Precision:  0.8698, Recall: 0.6806, ROC: 0.9254\n",
      "Epoch: 460, Loss: 0.0857, Accuracy: 0.9804, F1: 0.7720, Precision:  0.8765, Recall: 0.6898, ROC: 0.9259\n",
      "Epoch: 470, Loss: 0.0853, Accuracy: 0.9807, F1: 0.7752, Precision:  0.8772, Recall: 0.6944, ROC: 0.9265\n",
      "Epoch: 480, Loss: 0.0848, Accuracy: 0.9809, F1: 0.7795, Precision:  0.8736, Recall: 0.7037, ROC: 0.9271\n",
      "Epoch: 490, Loss: 0.0844, Accuracy: 0.9813, F1: 0.7835, Precision:  0.8837, Recall: 0.7037, ROC: 0.9270\n",
      "Epoch: 500, Loss: 0.0842, Accuracy: 0.9818, F1: 0.7897, Precision:  0.8851, Recall: 0.7130, ROC: 0.9278\n"
     ]
    }
   ],
   "source": [
    "num_features = 10\n",
    "num_classes = 2\n",
    "num_heads = 1\n",
    "hidden_dim = 128\n",
    "\n",
    "model = GAT(num_features, num_classes, num_heads, hidden_dim).to(device)\n",
    "\n",
    "num_epochs = 1000\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    loss = train(model, data_graph_scaled_filtered, optimizer, scheduler)\n",
    "    acc, f1, precision, recall, roc = test(model, data_graph_scaled_filtered)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {precision: .4f}, Recall: {recall:.4f}, ROC: {roc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Random Forest train_idx with top nodes masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10450"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_idx_removed = [x for x in train_idx if x not in nodes_to_remove]\n",
    "len(train_idx_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = selected_nodes.loc[train_idx_removed]\n",
    "y_train = selected_labels.loc[train_idx_removed]\n",
    "\n",
    "X_test = selected_nodes.loc[test_idx]\n",
    "y_test = selected_labels.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.98\n",
      "F1:        0.7606382978723405\n",
      "Precision: 0.89375\n",
      "Recall:    0.6620370370370371\n",
      "ROC AUC:   0.9439219360583739\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf.fit(X_train.reset_index(drop=True), y_train.reset_index(drop=True))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "precision = precision_score(y_test, y_pred, average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='binary')\n",
    "rocauc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"F1:       \", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:   \", recall)\n",
    "print(\"ROC AUC:  \", rocauc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f70ba4fa670>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "rf = [0.7680, 0.7606, 0.7580, 0.7560, 0.7683, 0.7459, 0.7444]\n",
    "gcn = [0.7493, 0.7526, 0.7268, 0.7198, 0.6337, 0.4342, 0.1212]\n",
    "gat = [0.7778, 0.7897, 0.7634, 0.7759, 0.7385, 0.4143, 0.1293]\n",
    "\n",
    "x = [0, 50, 100, 150, 200, 250, 300]\n",
    "\n",
    "plt.plot(x, rf, '-o', label='Random Forest')\n",
    "plt.plot(x, gcn,'-o',  label='GCN')\n",
    "plt.plot(x, gat, '-o', label='GAT')\n",
    "\n",
    "plt.xlabel('Masked nodes during training')\n",
    "plt.xticks([0, 50, 100, 150, 200, 250, 300])\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('Performance vs Number of Nodes Masked.')\n",
    "plt.legend(loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
