{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Dropout\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, GATConv, DenseGCNConv, GCNConv, GraphConv\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, accuracy_score\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ref: https://medium.com/stanford-cs224w/fraud-detection-with-gat-edac49bda1a0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate txId:  0\n"
     ]
    }
   ],
   "source": [
    "# import data \n",
    "df_features = pd.read_csv('../data/elliptic_txs_features.csv', header=None)\n",
    "df_edges = pd.read_csv(\"../data/elliptic_txs_edgelist.csv\")\n",
    "df_classes =  pd.read_csv(\"../data/elliptic_txs_classes.csv\")\n",
    "\n",
    "df_classes['class'] = df_classes['class'].map({'unknown': 2, '1':1, '2':0})\n",
    "\n",
    "# merging dataframes\n",
    "df_merge = df_features.merge(df_classes, how='left', right_on=\"txId\", left_on=0)\n",
    "df_merge.drop(0, axis=1, inplace=True)\n",
    "\n",
    "# check if there are duplicate txId\n",
    "print(\"Number of duplicate txId: \", df_merge.duplicated(subset=['txId']).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_step</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.171469</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>-0.167933</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>230425980</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>-0.167948</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>5530458</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>-0.168576</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>232022460</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782742</td>\n",
       "      <td>12.414558</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>-0.115831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>232438397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>0.041399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "      <td>230460314</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_step         2         3         4          5         6         7  \\\n",
       "0          1 -0.171469 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "1          1 -0.171484 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "2          1 -0.172107 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "3          1  0.163054  1.963790 -0.646376  12.409294 -0.063725  9.782742   \n",
       "4          1  1.011523 -0.081127 -1.201369   1.153668  0.333276  1.312656   \n",
       "\n",
       "           8         9        10  ...       159       160       161       162  \\\n",
       "0  -0.061584 -0.162097 -0.167933  ...  1.461330  1.461369  0.018279 -0.087490   \n",
       "1  -0.061584 -0.162112 -0.167948  ... -0.979074 -0.978556  0.018279 -0.087490   \n",
       "2  -0.061584 -0.162749 -0.168576  ... -0.979074 -0.978556 -0.098889 -0.106715   \n",
       "3  12.414558 -0.163645 -0.115831  ...  0.241128  0.241406  1.072793  0.085530   \n",
       "4  -0.061584 -0.163523  0.041399  ...  0.517257  0.579382  0.018279  0.277775   \n",
       "\n",
       "        163       164       165       166       txId  class  \n",
       "0 -0.131155 -0.097524 -0.120613 -0.119792  230425980      2  \n",
       "1 -0.131155 -0.097524 -0.120613 -0.119792    5530458      2  \n",
       "2 -0.131155 -0.183671 -0.120613 -0.119792  232022460      2  \n",
       "3 -0.131155  0.677799 -0.120613 -0.119792  232438397      0  \n",
       "4  0.326394  1.293750  0.178136  0.179117  230460314      2  \n",
       "\n",
       "[5 rows x 168 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(234355, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rename column 0 to time_step\n",
    "df_merge.rename(columns={1: 'time_step'}, inplace=True)\n",
    "display(df_merge.head())\n",
    "display(df_edges.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split dataset masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of edge index is torch.Size([2, 234355])\n"
     ]
    }
   ],
   "source": [
    "edges = df_edges.copy()\n",
    "\n",
    "# Setup trans ID to node ID mapping\n",
    "nodes = df_merge['txId'].values\n",
    "map_id = {j:i for i,j in enumerate(nodes)} # mapping nodes to indexes\n",
    "\n",
    "# Map transction IDs to node Ids\n",
    "edges.txId1 = edges.txId1.map(map_id) #get nodes idx1 from edges list and filtered data\n",
    "edges.txId2 = edges.txId2.map(map_id)\n",
    "edges = edges.astype(int)\n",
    "\n",
    "# Reformat and convert to tensor\n",
    "edge_index = np.array(edges.values).T \n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
    "\n",
    "print(\"shape of edge index is {}\".format(edge_index.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique= [2 0 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_step</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.171469</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162097</td>\n",
       "      <td>-0.167933</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.171484</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162112</td>\n",
       "      <td>-0.167948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.673103</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.172107</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.162749</td>\n",
       "      <td>-0.168576</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439728</td>\n",
       "      <td>-0.979074</td>\n",
       "      <td>-0.978556</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.106715</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.183671</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.163054</td>\n",
       "      <td>1.963790</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>12.409294</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>9.782742</td>\n",
       "      <td>12.414558</td>\n",
       "      <td>-0.163645</td>\n",
       "      <td>-0.115831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>1.072793</td>\n",
       "      <td>0.085530</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>0.677799</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.011523</td>\n",
       "      <td>-0.081127</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>1.153668</td>\n",
       "      <td>0.333276</td>\n",
       "      <td>1.312656</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163523</td>\n",
       "      <td>0.041399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400422</td>\n",
       "      <td>0.517257</td>\n",
       "      <td>0.579382</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.277775</td>\n",
       "      <td>0.326394</td>\n",
       "      <td>1.293750</td>\n",
       "      <td>0.178136</td>\n",
       "      <td>0.179117</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   time_step         2         3         4          5         6         7  \\\n",
       "0          1 -0.171469 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "1          1 -0.171484 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "2          1 -0.172107 -0.184668 -1.201369  -0.121970 -0.043875 -0.113002   \n",
       "3          1  0.163054  1.963790 -0.646376  12.409294 -0.063725  9.782742   \n",
       "4          1  1.011523 -0.081127 -1.201369   1.153668  0.333276  1.312656   \n",
       "\n",
       "           8         9        10  ...       158       159       160       161  \\\n",
       "0  -0.061584 -0.162097 -0.167933  ... -0.600999  1.461330  1.461369  0.018279   \n",
       "1  -0.061584 -0.162112 -0.167948  ...  0.673103 -0.979074 -0.978556  0.018279   \n",
       "2  -0.061584 -0.162749 -0.168576  ...  0.439728 -0.979074 -0.978556 -0.098889   \n",
       "3  12.414558 -0.163645 -0.115831  ... -0.613614  0.241128  0.241406  1.072793   \n",
       "4  -0.061584 -0.163523  0.041399  ... -0.400422  0.517257  0.579382  0.018279   \n",
       "\n",
       "        162       163       164       165       166  class  \n",
       "0 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792      2  \n",
       "1 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792      2  \n",
       "2 -0.106715 -0.131155 -0.183671 -0.120613 -0.119792      2  \n",
       "3  0.085530 -0.131155  0.677799 -0.120613 -0.119792      0  \n",
       "4  0.277775  0.326394  1.293750  0.178136  0.179117      2  \n",
       "\n",
       "[5 rows x 167 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "node_features = df_merge.drop(['txId'], axis=1).copy()\n",
    "print(\"unique=\",node_features[\"class\"].unique())\n",
    "\n",
    "# Retain known vs unknown IDs\n",
    "all_classified_idx = node_features['class'].loc[node_features['class']!=2].index # filter on known labels\n",
    "all_unclassified_idx = node_features['class'].loc[node_features['class']==2].index\n",
    "all_classified_illicit_idx = node_features['class'].loc[node_features['class']==1].index # filter on illicit labels\n",
    "all_classified_licit_idx = node_features['class'].loc[node_features['class']==0].index # filter on licit labels\n",
    "\n",
    "# node_features = node_features.drop(columns=[0, 1, 'class'])\n",
    "display(node_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_classified_idx.shape= (29894,)\n",
      "test_classified_idx.shape= (16670,)\n"
     ]
    }
   ],
   "source": [
    "train_classified_idx = node_features.loc[(node_features['time_step'] <= 34) & (node_features['class'] != 2)].index\n",
    "test_classified_idx = node_features.loc[(node_features['time_step'] > 34) & (node_features['class'] != 2)].index\n",
    "print(\"train_classified_idx.shape=\",train_classified_idx.shape)\n",
    "print(\"test_classified_idx.shape=\",test_classified_idx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the train and test indices as csv, integer\n",
    "np.savetxt(\"../data/index/train_classified_idx.csv\", train_classified_idx, delimiter=\",\", fmt='%d')\n",
    "np.savetxt(\"../data/index/test_classified_idx.csv\", test_classified_idx, delimiter=\",\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_features.drop(columns=['time_step'], inplace=True)\n",
    "node_features.drop(columns=['class'], inplace=True)\n",
    "\n",
    "# Convert to tensor\n",
    "node_features_t = torch.tensor(np.array(node_features.values, dtype=np.double), dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[203769, 166], edge_index=[2, 234355], edge_attr=[234355], y=[203769], train_idx=Int64Index([     3,      9,     10,     11,     16,     17,     25,     27,\n",
       "                29,     30,\n",
       "            ...\n",
       "            136232, 136233, 136234, 136236, 136239, 136241, 136243, 136249,\n",
       "            136250, 136258],\n",
       "           dtype='int64', length=29894), test_idx=Int64Index([136276, 136277, 136278, 136279, 136280, 136282, 136285, 136287,\n",
       "            136288, 136291,\n",
       "            ...\n",
       "            203727, 203730, 203736, 203740, 203750, 203752, 203754, 203759,\n",
       "            203763, 203766],\n",
       "           dtype='int64', length=16670))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define labels\n",
    "labels = df_merge['class'].values\n",
    "\n",
    "#create weights tensor with same shape of edge_index\n",
    "weights = torch.tensor([1]* edge_index.shape[1] , dtype=torch.double) \n",
    "\n",
    "# Do train test split on classified_ids\n",
    "train_idx = train_classified_idx\n",
    "test_idx = test_classified_idx\n",
    "\n",
    "# Create pyG dataset\n",
    "data_graph = Data(x=node_features_t.float(), edge_index=edge_index, edge_attr=weights, \n",
    "                               y=torch.tensor(labels, dtype=torch.long))\n",
    "\n",
    "# Add in the train and valid idx\n",
    "data_graph.train_idx = train_idx\n",
    "data_graph.test_idx = test_idx\n",
    "data_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STAGN Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class TGCN(torch.nn.Module):\n",
    "    r\"\"\"An implementation of the Temporal Graph Convolutional Gated Recurrent Cell.\n",
    "    For details see this paper: `\"T-GCN: A Temporal Graph ConvolutionalNetwork for\n",
    "    Traffic Prediction.\" <https://arxiv.org/abs/1811.05320>`_\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        improved (bool): Stronger self loops. Default is False.\n",
    "        cached (bool): Caching the message weights. Default is False.\n",
    "        add_self_loops (bool): Adding self-loops for smoothing. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        improved: bool = False,\n",
    "        cached: bool = False,\n",
    "        add_self_loops: bool = True,\n",
    "    ):\n",
    "        super(TGCN, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "\n",
    "        self._create_parameters_and_layers()\n",
    "\n",
    "    def _create_update_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_z = GCNConv(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            improved=self.improved,\n",
    "            cached=self.cached,\n",
    "            add_self_loops=self.add_self_loops,\n",
    "        )\n",
    "\n",
    "        self.linear_z = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_reset_gate_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_r = GCNConv(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            improved=self.improved,\n",
    "            cached=self.cached,\n",
    "            add_self_loops=self.add_self_loops,\n",
    "        )\n",
    "\n",
    "        self.linear_r = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_candidate_state_parameters_and_layers(self):\n",
    "\n",
    "        self.conv_h = GCNConv(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            improved=self.improved,\n",
    "            cached=self.cached,\n",
    "            add_self_loops=self.add_self_loops,\n",
    "        )\n",
    "\n",
    "        self.linear_h = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_parameters_and_layers(self):\n",
    "        self._create_update_gate_parameters_and_layers()\n",
    "        self._create_reset_gate_parameters_and_layers()\n",
    "        self._create_candidate_state_parameters_and_layers()\n",
    "\n",
    "    def _set_hidden_state(self, X, H):\n",
    "        if H is None:\n",
    "            H = torch.zeros(X.shape[0], self.out_channels).to(X.device)\n",
    "        return H\n",
    "\n",
    "    def _calculate_update_gate(self, X, edge_index, edge_weight, H):\n",
    "        Z = torch.cat([self.conv_z(X, edge_index, edge_weight), H], axis=1)\n",
    "        Z = self.linear_z(Z)\n",
    "        Z = torch.sigmoid(Z)\n",
    "        return Z\n",
    "\n",
    "    def _calculate_reset_gate(self, X, edge_index, edge_weight, H):\n",
    "        R = torch.cat([self.conv_r(X, edge_index, edge_weight), H], axis=1)\n",
    "        R = self.linear_r(R)\n",
    "        R = torch.sigmoid(R)\n",
    "        return R\n",
    "\n",
    "    def _calculate_candidate_state(self, X, edge_index, edge_weight, H, R):\n",
    "        H_tilde = torch.cat([self.conv_h(X, edge_index, edge_weight), H * R], axis=1)\n",
    "        H_tilde = self.linear_h(H_tilde)\n",
    "        H_tilde = torch.tanh(H_tilde)\n",
    "        return H_tilde\n",
    "\n",
    "    def _calculate_hidden_state(self, Z, H, H_tilde):\n",
    "        H = Z * H + (1 - Z) * H_tilde\n",
    "        return H\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        X: torch.FloatTensor,\n",
    "        edge_index: torch.LongTensor,\n",
    "        edge_weight: torch.FloatTensor = None,\n",
    "        H: torch.FloatTensor = None,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Making a forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph. If the hidden state matrix is not present\n",
    "        when the forward pass is called it is initialized with zeros.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
    "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
    "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
    "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
    "\n",
    "        Return types:\n",
    "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
    "        \"\"\"\n",
    "        H = self._set_hidden_state(X, H)\n",
    "        Z = self._calculate_update_gate(X, edge_index, edge_weight, H)\n",
    "        R = self._calculate_reset_gate(X, edge_index, edge_weight, H)\n",
    "        H_tilde = self._calculate_candidate_state(X, edge_index, edge_weight, H, R)\n",
    "        H = self._calculate_hidden_state(Z, H, H_tilde)\n",
    "        return H\n",
    "\n",
    "\n",
    "class TGCN2(torch.nn.Module):\n",
    "    r\"\"\"An implementation THAT SUPPORTS BATCHES of the Temporal Graph Convolutional Gated Recurrent Cell.\n",
    "    For details see this paper: `\"T-GCN: A Temporal Graph ConvolutionalNetwork for\n",
    "    Traffic Prediction.\" <https://arxiv.org/abs/1811.05320>`_\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Number of input features.\n",
    "        out_channels (int): Number of output features.\n",
    "        batch_size (int): Size of the batch.\n",
    "        improved (bool): Stronger self loops. Default is False.\n",
    "        cached (bool): Caching the message weights. Default is False.\n",
    "        add_self_loops (bool): Adding self-loops for smoothing. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, \n",
    "                 batch_size: int,  # this entry is unnecessary, kept only for backward compatibility\n",
    "                 improved: bool = False, cached: bool = False, \n",
    "                 add_self_loops: bool = True):\n",
    "        super(TGCN2, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.improved = improved\n",
    "        self.cached = cached\n",
    "        self.add_self_loops = add_self_loops\n",
    "        self.batch_size = batch_size  # not needed\n",
    "        self._create_parameters_and_layers()\n",
    "\n",
    "    def _create_update_gate_parameters_and_layers(self):\n",
    "        self.conv_z = GCNConv(in_channels=self.in_channels,  out_channels=self.out_channels, improved=self.improved,\n",
    "                              cached=self.cached, add_self_loops=self.add_self_loops )\n",
    "        self.linear_z = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_reset_gate_parameters_and_layers(self):\n",
    "        self.conv_r = GCNConv(in_channels=self.in_channels, out_channels=self.out_channels, improved=self.improved,\n",
    "                              cached=self.cached, add_self_loops=self.add_self_loops )\n",
    "        self.linear_r = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_candidate_state_parameters_and_layers(self):\n",
    "        self.conv_h = GCNConv(in_channels=self.in_channels, out_channels=self.out_channels, improved=self.improved,\n",
    "                              cached=self.cached, add_self_loops=self.add_self_loops )\n",
    "        self.linear_h = torch.nn.Linear(2 * self.out_channels, self.out_channels)\n",
    "\n",
    "    def _create_parameters_and_layers(self):\n",
    "        self._create_update_gate_parameters_and_layers()\n",
    "        self._create_reset_gate_parameters_and_layers()\n",
    "        self._create_candidate_state_parameters_and_layers()\n",
    "\n",
    "    def _set_hidden_state(self, X, H):\n",
    "        if H is None:\n",
    "            # can infer batch_size from X.shape, because X is [B, N, F]\n",
    "            H = torch.zeros(X.shape[0], X.shape[1], self.out_channels).to(X.device) #(b, 207, 32)\n",
    "        return H\n",
    "\n",
    "    def _calculate_update_gate(self, X, edge_index, edge_weight, H):\n",
    "        Z = torch.cat([self.conv_z(X, edge_index, edge_weight), H], axis=2) # (b, 207, 64)\n",
    "        Z = self.linear_z(Z) # (b, 207, 32)\n",
    "        Z = torch.sigmoid(Z)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def _calculate_reset_gate(self, X, edge_index, edge_weight, H):\n",
    "        R = torch.cat([self.conv_r(X, edge_index, edge_weight), H], axis=2) # (b, 207, 64)\n",
    "        R = self.linear_r(R) # (b, 207, 32)\n",
    "        R = torch.sigmoid(R)\n",
    "\n",
    "        return R\n",
    "\n",
    "    def _calculate_candidate_state(self, X, edge_index, edge_weight, H, R):\n",
    "        H_tilde = torch.cat([self.conv_h(X, edge_index, edge_weight), H * R], axis=2) # (b, 207, 64)\n",
    "        H_tilde = self.linear_h(H_tilde) # (b, 207, 32)\n",
    "        H_tilde = torch.tanh(H_tilde)\n",
    "\n",
    "        return H_tilde\n",
    "\n",
    "    def _calculate_hidden_state(self, Z, H, H_tilde):\n",
    "        H = Z * H + (1 - Z) * H_tilde   # # (b, 207, 32)\n",
    "        return H\n",
    "\n",
    "    def forward(self,X: torch.FloatTensor, edge_index: torch.LongTensor, edge_weight: torch.FloatTensor = None,\n",
    "                H: torch.FloatTensor = None ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Making a forward pass. If edge weights are not present the forward pass\n",
    "        defaults to an unweighted graph. If the hidden state matrix is not present\n",
    "        when the forward pass is called it is initialized with zeros.\n",
    "\n",
    "        Arg types:\n",
    "            * **X** *(PyTorch Float Tensor)* - Node features.\n",
    "            * **edge_index** *(PyTorch Long Tensor)* - Graph edge indices.\n",
    "            * **edge_weight** *(PyTorch Long Tensor, optional)* - Edge weight vector.\n",
    "            * **H** *(PyTorch Float Tensor, optional)* - Hidden state matrix for all nodes.\n",
    "\n",
    "        Return types:\n",
    "            * **H** *(PyTorch Float Tensor)* - Hidden state matrix for all nodes.\n",
    "        \"\"\"\n",
    "        H = self._set_hidden_state(X, H)\n",
    "        Z = self._calculate_update_gate(X, edge_index, edge_weight, H)\n",
    "        R = self._calculate_reset_gate(X, edge_index, edge_weight, H)\n",
    "        H_tilde = self._calculate_candidate_state(X, edge_index, edge_weight, H, R)\n",
    "        H = self._calculate_hidden_state(Z, H, H_tilde) # (b, 207, 32)\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = data_graph.num_node_features\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGCN(\n",
       "  (conv_z): GCNConv(166, 2)\n",
       "  (linear_z): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (conv_r): GCNConv(166, 2)\n",
       "  (linear_r): Linear(in_features=4, out_features=2, bias=True)\n",
       "  (conv_h): GCNConv(166, 2)\n",
       "  (linear_h): Linear(in_features=4, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tgcn = TGCN(in_channels=num_features, out_channels=2)\n",
    "num_epochs = 200\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model_tgcn.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    # out = out.reshape((data.x.shape[0]))\n",
    "    # TODO :use weighted cross entropy loss\n",
    "    loss = F.cross_entropy(out[data.train_idx], data.y[data.train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred_scores = out[data.test_idx]\n",
    "        pred = torch.argmax(pred_scores, dim=1)\n",
    "        y = data.y[data.test_idx]\n",
    "        acc = accuracy_score(y.cpu(), pred.cpu())\n",
    "        f1 = f1_score(y.cpu(), pred.cpu())\n",
    "        precision = precision_score(y.cpu(), pred.cpu())\n",
    "        recall = recall_score(y.cpu(), pred.cpu())\n",
    "        roc = roc_auc_score(y.cpu(), pred.cpu())\n",
    "        return acc, f1, precision, recall, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransactionGCN(torch.nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, out_feats, g, device):\n",
    "        super().__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_feats,\n",
    "                               allow_zero_in_degree=True)\n",
    "        self.conv2 = GraphConv(hidden_feats, out_feats,\n",
    "                               allow_zero_in_degree=True)\n",
    "        self.lin1 = torch.nn.Linear(in_feats, hidden_feats)\n",
    "        self.lin2 = torch.nn.Linear(hidden_feats, out_feats)\n",
    "\n",
    "        g.ndata['feat'] = torch.nn.init.xavier_uniform_(torch.empty(\n",
    "            g.num_nodes(), g.edata['feat'].shape[1])).to(torch.float32).to(device)\n",
    "        g.ndata['h'] = g.ndata['feat']\n",
    "        g.edata['x'] = g.edata['feat']\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        h1 = torch.relu(self.conv1(g, h))\n",
    "        e1 = torch.relu(self.lin1(e))\n",
    "        g.ndata['h'] = h1\n",
    "        g.edata['x'] = e1\n",
    "        g.apply_edges(\n",
    "            lambda edges: {'x': edges.src['h'] + edges.dst['h'] + edges.data['x']})\n",
    "\n",
    "        h2 = self.conv2(g, h1)\n",
    "        e2 = torch.relu(self.lin2(e1))\n",
    "        g.ndata['h'] = h2\n",
    "        g.edata['x'] = e2\n",
    "        g.apply_edges(\n",
    "            lambda edges: {'x': edges.src['h'] + edges.dst['h'] + edges.data['x']})\n",
    "        return g.ndata['h'], g.edata['x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stagn_2d_model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_windows_dim: int,\n",
    "        feat_dim: int,\n",
    "        num_classes: int,\n",
    "        attention_hidden_dim: int,\n",
    "        g: dgl.DGLGraph,\n",
    "        filter_sizes: tuple = (2, 2),\n",
    "        num_filters: int = 64,\n",
    "        in_channels: int = 1,\n",
    "        device=\"cpu\"\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the STAGN-2d model\n",
    "\n",
    "        Args:\n",
    "        :param time_windows_dim (int): length of time windows\n",
    "        :param feat_dim (int): feature dimension\n",
    "        :param num_classes (int): number of classes\n",
    "        :param attention_hidden_dim (int): attention hidden dimenstion\n",
    "        :param g (dgl.DGLGraph): dgl graph for gcn embeddings\n",
    "        :param filter_sizes (tuple, optional): cnn filter size\n",
    "        :param num_filters (int, optional): number of hidden channels\n",
    "        :param in_channels (int, optional): number of in channels\n",
    "        :param device (str, optional): where to train the model\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.time_windows_dim = time_windows_dim\n",
    "        self.feat_dim = feat_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        self.graph = g.to(device)\n",
    "\n",
    "        # attention layer\n",
    "        self.attention_W = nn.Parameter(torch.Tensor(\n",
    "            self.feat_dim, self.attention_hidden_dim).uniform_(0., 1.))\n",
    "        self.attention_U = nn.Parameter(torch.Tensor(\n",
    "            self.feat_dim, self.attention_hidden_dim).uniform_(0., 1.))\n",
    "        self.attention_V = nn.Parameter(torch.Tensor(\n",
    "            self.attention_hidden_dim, 1).uniform_(0., 1.))\n",
    "\n",
    "        # cnn layer\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=num_filters,\n",
    "            kernel_size=filter_sizes,\n",
    "            padding='same'\n",
    "        )\n",
    "\n",
    "        # FC layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linears1 = nn.Sequential(\n",
    "            nn.LazyLinear(256),\n",
    "            nn.ReLU(),\n",
    "            nn.LazyLinear(24),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.linears2 = nn.LazyLinear(self.num_classes)\n",
    "\n",
    "        # gnn for transaction graph\n",
    "        self.gcn = TransactionGCN(\n",
    "            g.edata['feat'].shape[1], 128, 8, g, device)\n",
    "\n",
    "    def attention_layer(\n",
    "        self,\n",
    "        X: torch.Tensor\n",
    "    ):\n",
    "        self.output_att = []\n",
    "        # input_att = torch.split(X, self.time_windows_dim, dim=1)\n",
    "        input_att = torch.split(X, 1, dim=1)  # 第二个参数是split_size!\n",
    "        for index, x_i in enumerate(input_att):\n",
    "            # print(f\"x_i shape: {x_i.shape}\")\n",
    "            x_i = x_i.reshape(-1, self.feat_dim)\n",
    "            c_i = self.attention(x_i, input_att, index)\n",
    "            inp = torch.concat([x_i, c_i], axis=1)\n",
    "            self.output_att.append(inp)\n",
    "\n",
    "        input_conv = torch.reshape(torch.concat(self.output_att, axis=1),\n",
    "                                   [-1, self.time_windows_dim, self.feat_dim*2])\n",
    "\n",
    "        self.input_conv_expanded = torch.unsqueeze(input_conv, 1)\n",
    "\n",
    "        return self.input_conv_expanded\n",
    "\n",
    "    def cnn_layer(\n",
    "        self,\n",
    "        input: torch.Tensor\n",
    "    ):\n",
    "        if len(input.shape) == 3:\n",
    "            self.input_conv_expanded = torch.unsqueeze(input, 1)\n",
    "        elif len(input.shape) == 4:\n",
    "            self.input_conv_expanded = input\n",
    "        else:\n",
    "            print(\"Wrong conv input shape!\")\n",
    "\n",
    "        self.input_conv_expanded = F.relu(self.conv(input))\n",
    "\n",
    "        return self.input_conv_expanded\n",
    "\n",
    "    def attention(self, x_i, x, index):\n",
    "        e_i = []\n",
    "        c_i = []\n",
    "\n",
    "        for i in range(len(x)):\n",
    "            output = x[i]\n",
    "            output = output.reshape(-1, self.feat_dim)\n",
    "            att_hidden = torch.tanh(torch.add(torch.matmul(\n",
    "                x_i, self.attention_W), torch.matmul(output, self.attention_U)))\n",
    "            e_i_j = torch.matmul(att_hidden, self.attention_V)\n",
    "            e_i.append(e_i_j)\n",
    "\n",
    "        e_i = torch.concat(e_i, axis=1)\n",
    "        # print(f\"e_i shape: {e_i.shape}\")\n",
    "        alpha_i = F.softmax(e_i, dim=1)\n",
    "        alpha_i = torch.split(alpha_i, 1, 1)  # !!!\n",
    "\n",
    "        for j, (alpha_i_j, output) in enumerate(zip(alpha_i, x)):\n",
    "            if j == index:\n",
    "                continue\n",
    "            else:\n",
    "                output = output.reshape(-1, self.feat_dim)\n",
    "                c_i_j = torch.multiply(alpha_i_j, output)\n",
    "                c_i.append(c_i_j)\n",
    "\n",
    "        c_i = torch.reshape(torch.concat(c_i, axis=1),\n",
    "                            [-1, self.time_windows_dim-1, self.feat_dim])\n",
    "        c_i = torch.sum(c_i, dim=1)\n",
    "        return c_i\n",
    "\n",
    "    def forward(self, X_nume, g):\n",
    "        # X shape be like: (batch_size, time_windows_dim, feat_dim)\n",
    "        out = self.attention_layer(X_nume)  # all, 1, 8, 10\n",
    "\n",
    "        out = self.cnn_layer(out)  # all, 64, 8, 10\n",
    "        node_embs, edge_embs = self.gcn(g, g.ndata['feat'], g.edata['feat'])\n",
    "\n",
    "        src_nds, dst_nds = g.edges()\n",
    "        src_feat = g.ndata['h'][src_nds]\n",
    "        dst_feat = g.ndata['h'][dst_nds]\n",
    "        # all, 3, embedding_dim\n",
    "        node_feats = torch.stack(\n",
    "            [src_feat, dst_feat, edge_embs], dim=1).view(X_nume.shape[0], -1)\n",
    "        out = self.flatten(out)\n",
    "        out = self.linears1(out)\n",
    "        out = torch.cat([out, node_feats], dim=1)\n",
    "        out = self.linears2(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCN Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-layer GCN\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes, hidden_channels=128):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "        self.dropout = Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, optimizer):\n",
    "    model.train()\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    # out = out.reshape((data.x.shape[0]))\n",
    "    # TODO :use weighted cross entropy loss\n",
    "    loss = F.cross_entropy(out[data.train_idx], data.y[data.train_idx])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index)\n",
    "        pred_scores = out[data.test_idx]\n",
    "        pred = torch.argmax(pred_scores, dim=1)\n",
    "        y = data.y[data.test_idx]\n",
    "        acc = accuracy_score(y.cpu(), pred.cpu())\n",
    "        f1 = f1_score(y.cpu(), pred.cpu())\n",
    "        precision = precision_score(y.cpu(), pred.cpu())\n",
    "        recall = recall_score(y.cpu(), pred.cpu())\n",
    "        roc = roc_auc_score(y.cpu(), pred.cpu())\n",
    "        return acc, f1, precision, recall, roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_features= 166\n"
     ]
    }
   ],
   "source": [
    "num_features = data_graph.num_node_features\n",
    "print(\"num_features=\",num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 1.3681, Accuracy: 0.9254, F1: 0.0000, Precision: 0.0000, Recall: 0.0000, ROC: 0.4949\n",
      "Epoch: 010, Loss: 0.5373, Accuracy: 0.9317, F1: 0.0656, Precision: 0.2941, Recall: 0.0369, ROC: 0.5154\n",
      "Epoch: 020, Loss: 0.4710, Accuracy: 0.8797, F1: 0.2521, Precision: 0.2114, Recall: 0.3121, ROC: 0.6156\n",
      "Epoch: 030, Loss: 0.4366, Accuracy: 0.8810, F1: 0.2587, Precision: 0.2173, Recall: 0.3195, ROC: 0.6198\n",
      "Epoch: 040, Loss: 0.4007, Accuracy: 0.8667, F1: 0.2892, Precision: 0.2212, Recall: 0.4174, ROC: 0.6576\n",
      "Epoch: 050, Loss: 0.3832, Accuracy: 0.8739, F1: 0.2975, Precision: 0.2331, Recall: 0.4109, ROC: 0.6585\n",
      "Epoch: 060, Loss: 0.3559, Accuracy: 0.8694, F1: 0.3043, Precision: 0.2326, Recall: 0.4395, ROC: 0.6694\n",
      "Epoch: 070, Loss: 0.3364, Accuracy: 0.8550, F1: 0.3004, Precision: 0.2188, Recall: 0.4792, ROC: 0.6802\n",
      "Epoch: 080, Loss: 0.3178, Accuracy: 0.8611, F1: 0.3032, Precision: 0.2249, Recall: 0.4654, ROC: 0.6770\n",
      "Epoch: 090, Loss: 0.3056, Accuracy: 0.8512, F1: 0.3084, Precision: 0.2209, Recall: 0.5106, ROC: 0.6928\n",
      "Epoch: 100, Loss: 0.2954, Accuracy: 0.8496, F1: 0.3117, Precision: 0.2218, Recall: 0.5245, ROC: 0.6983\n",
      "Epoch: 110, Loss: 0.2867, Accuracy: 0.8488, F1: 0.3148, Precision: 0.2231, Recall: 0.5346, ROC: 0.7026\n",
      "Epoch: 120, Loss: 0.2708, Accuracy: 0.8516, F1: 0.3214, Precision: 0.2286, Recall: 0.5411, ROC: 0.7071\n",
      "Epoch: 130, Loss: 0.2632, Accuracy: 0.8455, F1: 0.3182, Precision: 0.2230, Recall: 0.5549, ROC: 0.7103\n",
      "Epoch: 140, Loss: 0.2597, Accuracy: 0.8507, F1: 0.3235, Precision: 0.2292, Recall: 0.5494, ROC: 0.7105\n",
      "Epoch: 150, Loss: 0.2496, Accuracy: 0.8459, F1: 0.3241, Precision: 0.2266, Recall: 0.5688, ROC: 0.7170\n",
      "Epoch: 160, Loss: 0.2416, Accuracy: 0.8500, F1: 0.3279, Precision: 0.2312, Recall: 0.5633, ROC: 0.7166\n",
      "Epoch: 170, Loss: 0.2356, Accuracy: 0.8455, F1: 0.3306, Precision: 0.2301, Recall: 0.5873, ROC: 0.7254\n",
      "Epoch: 180, Loss: 0.2320, Accuracy: 0.8506, F1: 0.3309, Precision: 0.2333, Recall: 0.5688, ROC: 0.7195\n",
      "Epoch: 190, Loss: 0.2264, Accuracy: 0.8496, F1: 0.3316, Precision: 0.2331, Recall: 0.5743, ROC: 0.7215\n",
      "Epoch: 200, Loss: 0.2278, Accuracy: 0.8443, F1: 0.3317, Precision: 0.2300, Recall: 0.5946, ROC: 0.7282\n"
     ]
    }
   ],
   "source": [
    "model = GCN(num_features, 2).to(device)\n",
    "num_epochs = 200\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    loss = train(model, data_graph, optimizer)\n",
    "    acc, f1, precision, recall, roc = test(model, data_graph)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, ROC: {roc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8443, F1: 0.3317, Precision: 0.2300, Recall: 0.5946, ROC: 0.7282\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "acc, f1, precision, recall, roc = test(model, data_graph)\n",
    "print(f\"Accuracy: {acc:.4f}, F1: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, ROC: {roc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
